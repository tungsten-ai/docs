{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tungsten","text":""},{"location":"#what-is-tungsten","title":"What is Tungsten?","text":"<p>Tungsten is an open-source ML containerization tool/platform with a focus on developer productivity and collaboration.</p> <p>Tungsten builds a versatile and standardized container for your model. Once built, it can run as a REST API server, GUI application, CLI application, serverless function, or scriptable Python function.</p> <p>We also provide a platform for managing and sharing your ML models. It currently supports remote execution, test automation as well as basic versioning feature.</p> <p> </p>"},{"location":"#tungsten-model","title":"Tungsten Model","text":"<p>The Tungsten model packages up everything required to run your model, and exposes a standardized API to support convinient features.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy: Requires only a few lines of Python code.</li> <li>Versatile: Support multiple usages:<ul> <li>RESTful API server</li> <li>GUI application</li> <li>Serverless function</li> <li>CLI application (coming soon)</li> <li>Python function (coming soon)</li> </ul> </li> <li>Abstraction: User-defined JSON input/output.</li> <li>Standardized: Supports advanced workflows.</li> <li>Scalable: Supports adaptive batching and clustering (coming soon).</li> </ul> <p>See Tungsten Model - Getting Started to learn more.</p>"},{"location":"#take-the-tour","title":"Take the tour","text":""},{"location":"#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Building a Tungsten model is easy. All you have to do is write a simple <code>tungsten_model.py</code> like below:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\nprompt: str\nclass Output(io.BaseIO):\nimage: io.Image\n@model.config(\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ndescription=\"Text to image\"\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> <p>Now, you can start a build process with the following command: <pre><code>$ tungsten build\n\n\u2705 Successfully built tungsten model: 'text-to-image:latest'\n</code></pre></p>"},{"location":"#run-it-as-a-restful-api-server","title":"Run it as a RESTful API server","text":"<p>You can start a prediction with a REST API call.</p> <p>Start a server:</p> <pre><code>$ docker run -p 3000:3000 --gpus all text-to-image:latest\n\nINFO:     Setting up the model\nINFO:     Getting inputs from the input queue\nINFO:     Starting the prediction service\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>Send a prediction request with a JSON payload:</p> <pre><code>$ curl -X 'POST' 'http://localhost:3000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"prompt\": \"a professional photograph of an astronaut riding a horse\"}]'\n{\n    \"status\": \"success\",\n    \"outputs\": [{\"image\": \"data:image/png;base64,...\"}],\n    \"error_message\": null\n}\n</code></pre>"},{"location":"#run-it-as-a-gui-application","title":"Run it as a GUI application","text":"<p>If you need a more user-friendly way to make predictions, start a GUI app with the following command:</p> <pre><code>$ tungsten demo text-to-image:latest -p 8080\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n</code></pre> <p></p>"},{"location":"#run-it-as-a-serverless-function","title":"Run it as a serverless function","text":"<p>We support remote, serverless executions via a Tungsten server.</p> <p>Push a model:</p> <pre><code>$ tungsten push exampleuser/exampleproject -n text-to-image:latest\n\n\u2705 Successfully pushed to 'https://server.tungsten-ai.com'\n</code></pre> <p>Now, you can start a remote prediction in the Tungsten server:</p> <p></p> <p>See Tungsten Model - Getting Started to learn more.</p>"},{"location":"#tungsten-server","title":"Tungsten Server","text":"<p>The Tungsten server enables running a platform where you can store, run, and test models.</p>"},{"location":"#key-features_1","title":"Key Features","text":"<ul> <li>Function-as-a-Service (FaaS)</li> <li>Scale with your own GPU/CPU devices</li> <li>Project management</li> <li>Automated testing for CI/CD (comming soon)</li> </ul> <p>See Tungsten Server - Getting Started to learn more.</p>"},{"location":"#take-the-tour_1","title":"Take the tour","text":""},{"location":"#function-as-a-service-faas","title":"Function-as-a-Service (FaaS)","text":"<p>The Tungsten server supports executing models as serverless functions.</p> <p>In a browser, you can test any uploaded model:</p> <p></p> <p>Also, it is possible to make a prediction through the Tungsten server's REST API:</p> <pre><code>$ curl -X 'POST' \\\n'https://server.tungsten-ai.com/api/v1/projects/tungsten/text-to-image/models/2910c07e/predict' \\\n-H 'Authorization: ************' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\"input\": {\"prompt\": \"a professional photograph of an astronaut riding a horse\"}}'\n{\n  \"id\": \"c88e7de9\",\n  \"status\": \"running\",\n}\n$ curl -X 'GET' \\\n'https://server.tungsten-ai.com/api/v1/predictions/c88e7de9' \\\n-H 'Authorization: ************' \\\n-H 'accept: application/json' \\\n{\n  \"output\": {\n    \"image\": \"https://server.tungsten-ai.com/api/v1/files/1/93fd2ac4/output.png\"\n  },\n  \"status\": \"success\"\n}\n</code></pre>"},{"location":"#scale-with-your-own-gpucpu-devices","title":"Scale with your own GPU/CPU devices","text":"<p>You can easily scale serverless infrastructure with your Tungsten runners.</p> <p>Register one or more runners with the following command:</p> <pre><code>$ tungsten-runner register\n\nEnter runner mode (pipeline, prediction) [prediction]: prediction\nEnter URL of the tungsten server: https://server.tungsten-ai.com\nEnter registration token: C6r5rp2PhfdXbJtFbBMhifgLDhagAc\nEnter runner name [mydesktop]: myrunner \nEnter tags (comma separated) []: NVIDIA-A100\nEnter GPU index to use []: 0\nRunner 'myrunner' is registered - id: 245\nUpdated runner config\n</code></pre> <p>Then, start the runners to fetch jobs</p> <pre><code>$ tungsten-runner run\n\nRunner 0   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n                      2023-04-21 16:59:49.184 | INFO     | Job 0f7c50867417456ebd1389cfb74e489f assigned\nRunner 1   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n</code></pre>"},{"location":"#project-management","title":"Project management","text":"<p>In a Tungsten server, you can organize models by grouping them into projects. </p> <p>Multiple settings are unified across all models in a project:</p> <ul> <li>Input/output schemas</li> <li>Evaluation metrics</li> <li>Test cases</li> <li>Test data</li> </ul> <p>So, it helps you compare models clearly.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Apache License 2.0.</p>"},{"location":"tungsten_model/getting_started/","title":"Getting Started","text":""},{"location":"tungsten_model/getting_started/#installation","title":"Installation","text":"<p>The first step is to install Tungstenkit.</p> <p>The prerequisites are:</p> <ul> <li>Python &gt;= 3.7</li> <li>Docker</li> <li>(Optional) nvidia-docker for running GPU models locally. It enables to use NVIDIA GPUs in a Docker container. However, you can build and push a GPU model without a GPU and nvidia-docker.</li> </ul> <p>If they are ready, you can install Tungstenkit as follows:</p> <pre><code>pip install tungstenkit\n</code></pre>"},{"location":"tungsten_model/getting_started/#run-an-example-model","title":"Run an example model","text":""},{"location":"tungsten_model/getting_started/#create-a-directory","title":"Create a directory","text":"<p>Let's start by creating a working directory: <pre><code>mkdir tungsten-getting-started\ncd tungsten-getting-started\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#write-tungsten_modelpy","title":"Write <code>tungsten_model.py</code>","text":"<p>To build a Tungsten model, you should define your input, output, setup &amp; predict functions, and dependencies in <code>tungsten_model.py</code> file.</p> <p>For example, you can define them for an image classification model like this: <pre><code>from typing import List\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str\n@model.config(\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\nself.model = MobileNetV2()\nself.model.load_state_dict(torch.load(\"mobilenetv2_weights.pth\"))\nself.model.eval()\nself.labels = MobileNet_V2_Weights.IMAGENET1K_V2.meta[\"categories\"]\nself.transforms = transforms.Compose(\n[\ntransforms.Resize((224, 224)),\ntransforms.PILToTensor(),\ntransforms.ConvertImageDtype(torch.float),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n]\n)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = self._preprocess(inputs)\nsoftmax = self.model(input_tensor).softmax(1)\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [self.labels[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\ndef _preprocess(self, inputs: List[Input]):\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [self.transforms(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nreturn input_tensor\n</code></pre> Copy that to a file <code>tungsten_model.py</code>.</p>"},{"location":"tungsten_model/getting_started/#prepare-the-build-environment","title":"Prepare the build environment","text":"<p>We should prepare files used in <code>setup</code> function of <code>Model</code> class in <code>tungsten_model.py</code>. Refering to the definition, <code>mobilenetv2_weights.pth</code> is required. Let's download it: <pre><code>curl -o mobilenetv2_weights.pth https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Now everything is ready. Let's start building a Tungsten model: <pre><code>$ tungsten build -n tungsten-example\n\n\u2705 Successfully built tungsten model: 'tungsten-example:e3a5de5616a743fe9021e2dcfe1cd19a' (also tagged as 'tungsten-example:latest')\n</code></pre></p> <p>Also you can see that the model is added to the list of models: <pre><code>$ tungsten models\n\nRepository        Tag                               Description       Model Class           Created              Docker Image ID\n----------------  --------------------------------  ----------------  --------------------  -------------------  -----------------\ntungsten-example  latest                            Blur after delay  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\ntungsten-example  e3a5de5616a743fe9021e2dcfe1cd19a  Blur after delay  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#run-locally","title":"Run locally","text":"<p>Now, you can run the model in your local machine.</p> <p>Tungstenkit provides multiple options for that.</p>"},{"location":"tungsten_model/getting_started/#option-1-an-interactive-web-demo","title":"Option 1: an interactive web demo","text":"<p><pre><code>tungsten demo tungsten-example -p 8080\n</code></pre> Visit http://localhost:8080 to check.</p>"},{"location":"tungsten_model/getting_started/#option-2-a-restful-api","title":"Option 2: a RESTful API","text":"<p><pre><code>tungsten serve tungsten-example -p 3000\n</code></pre> A Swagger documentation is automatically generated. You can find it in http://localhost:3000:</p> <p></p>"},{"location":"tungsten_model/getting_started/#run-remotely","title":"Run remotely","text":"<p>To do this, you should have an account and an entered project in a Tungsten server running at https://tungsten-ai.com.  </p> <p>If you have them, let's login first. <pre><code>tungsten login\n</code></pre></p> <p>Then, you can push the built model: <pre><code>tungsten push &lt;username&gt;/&lt;project name&gt;\n</code></pre></p> <p>Now you can find a new model is added to the project.</p> <p>Visit https://webapp.tungsten-ai.com in a browser and run it.</p> <p>Also, you can pull the model as follows: <pre><code>tungsten pull &lt;username&gt;/&lt;project name&gt;:&lt;model version&gt;\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#use-gpus","title":"Use GPUs","text":""},{"location":"tungsten_model/getting_started/#setup","title":"Setup","text":"<p>You can build and push a GPU model without any extra setup.</p> <p>However, for running GPU models locally, nvidia-docker should be installed. It enables to use NVIDIA GPUs in a Docker container.</p>"},{"location":"tungsten_model/getting_started/#update-tungsten_modelpy","title":"Update <code>tungsten_model.py</code>","text":"<p>Now modify the file <code>tungsten_model.py</code> to use GPUs.</p> <p><pre><code>from typing import List\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str\n@model.config(\ngpu=True,\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\nself.model = MobileNetV2()\nself.model.load_state_dict(torch.load(\"mobilenetv2_weights.pth\"))\nself.model.cuda()\nself.model.eval()\nself.labels = MobileNet_V2_Weights.IMAGENET1K_V2.meta[\"categories\"]\nself.transforms = transforms.Compose(\n[\ntransforms.Resize((224, 224)),\ntransforms.PILToTensor(),\ntransforms.ConvertImageDtype(torch.float),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n]\n)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = self._preprocess(inputs)\nsoftmax = self.model(input_tensor).softmax(1)\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [self.labels[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\ndef _preprocess(self, inputs: List[Input]):\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [self.transforms(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\ninput_tensor = input_tensor.cuda()\nreturn input_tensor\n</code></pre> As you see, just setting <code>gpu=True</code> in <code>model.config</code> decorator is enough.  Then, Tungstenkit automatically selects a compatible CUDA version and installs it in the container. Currently, the automatic cuda version inference is supported on <code>torch</code>, <code>torchvision</code>, <code>torchaudio</code>, and <code>tensorflow</code>.</p> <p>If you want to manually set the CUDA version, modify <code>model.config</code> decorator as follows.  <pre><code>@model.config(\ngpu=True,\ncuda_version=\"11.6\",\ndescription=\"Image classification model on GPU\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\n)\n</code></pre></p>"},{"location":"tungsten_model/inputs_and_outputs/","title":"Inputs and outputs","text":""},{"location":"tungsten_model/inputs_and_outputs/#the-baseio-class","title":"The BaseIO class","text":""},{"location":"tungsten_model/inputs_and_outputs/#files","title":"Files","text":""},{"location":"tungsten_model/inputs_and_outputs/#supported-types","title":"Supported types","text":""},{"location":"tungsten_model/tungsten_model/","title":"Tungsten model","text":""},{"location":"tungsten_model/tungsten_model/#declare-input-output-types","title":"Declare input &amp; output types","text":""},{"location":"tungsten_model/tungsten_model/#define-how-a-prediction-works","title":"Define how a prediction works","text":""},{"location":"tungsten_model/tungsten_model/#define-how-to-setup-a-model","title":"Define how to setup a model","text":""},{"location":"tungsten_model/tungsten_model/#configure-a-model","title":"Configure a model","text":""},{"location":"tungsten_server/getting_started/","title":"Getting Started","text":"<p>Comming soon</p>"}]}