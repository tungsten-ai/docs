{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tungstenkit: ML container made simple","text":"<p>Tungstenkit is ML containerization tool with a focus on developer productivity and versatility. </p> <p>Have you ever struggled to use models from github? You may have repeated tedious steps like: cuda/dependency problems, file handling, and scripting for testing.</p> <p>Standing on the shoulder of Docker, this project aims to make using ML models less painful by adding functionalities for typical use cases - REST API server, GUI, CLI, and Python script.</p> <p>With Tungstenkit, sharing and consuming ML models can be quick and enjoyable.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Requires only a few lines of Python code</li> <li>Build once, use everywhere:<ul> <li>REST API server</li> <li>GUI application</li> <li>CLI application</li> <li>Python function</li> </ul> </li> <li>Framework-agnostic and lightweight</li> <li>Pydantic input/output definitions with convenient file handling</li> <li>Supports batched prediction</li> <li>Supports clustering with distributed machines (coming soon)</li> </ul>"},{"location":"#take-the-tour","title":"Take the tour","text":""},{"location":"#requires-only-a-few-lines-of-python-code","title":"Requires only a few lines of python code","text":"<p>Building a Tungsten model is easy. All you have to do is write a simple <code>tungsten_model.py</code> like:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nprompt: str\nclass Output(BaseIO):\nimage: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ngpu_mem_gb=16,\n)\nclass TextToImageModel:\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> <p>Start a build process:</p> <pre><code>$ tungsten build . -n text-to-image\n\n\u2705 Successfully built tungsten model: 'text-to-image:e3a5de56'\n</code></pre> <p>Check the built image: <pre><code>$ tungsten models\n\nRepository        Tag       Create Time          Docker Image ID\n----------------  --------  -------------------  ---------------\ntext-to-image     latest    2023-04-26 05:23:58  830eb82f0fcd\ntext-to-image     e3a5de56  2023-04-26 05:23:58  830eb82f0fcd\n</code></pre></p>"},{"location":"#build-once-use-everywhere","title":"Build once, use everywhere","text":""},{"location":"#rest-api-server","title":"REST API server","text":"<p>Start a server:</p> <pre><code>$ tungsten serve text-to-image -p 3000\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>Send a prediction request with a JSON payload:</p> <pre><code>$ curl -X 'POST' 'http://localhost:3000/predictions' \\\n-H 'Accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"prompt\": \"a professional photograph of an astronaut riding a horse\"}]'\n{\n    \"prediction_id\": \"39c9eb6b\"\n}\n</code></pre> <p>Get the result: <pre><code>$ curl -X 'GET' 'http://localhost:3000/predictions/39c9eb6b' \\\n-H 'Accept: application/json'\n{\n    \"outputs\": [{\"image\": \"data:image/png;base64,...\"}],\n    \"status\": \"success\"\n}\n</code></pre></p>"},{"location":"#gui-application","title":"GUI application","text":"<p>If you need a more user-friendly way to make predictions, start a GUI app with the following command:</p> <pre><code>$ tungsten demo text-to-image -p 8080\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n</code></pre> <p></p>"},{"location":"#cli-application","title":"CLI application","text":"<p>Run a prediction in a terminal: <pre><code>$ tungsten predict text-to-image \\\n-i prompt=\"a professional photograph of an astronaut riding a horse\"\n{\n  \"image\": \"./output.png\"\n}\n</code></pre></p>"},{"location":"#python-function","title":"Python function","text":"<p>If you want to run a model in your Python application, use the Python API: <pre><code>&gt;&gt;&gt; from tungstenkit import models\n&gt;&gt;&gt; model = models.get(\"text-to-image\")\n&gt;&gt;&gt; model.predict(\n{\"prompt\": \"a professional photograph of an astronaut riding a horse\"}\n)\n{\"image\": PosixPath(\"./output.png\")}\n</code></pre></p>"},{"location":"#framework-agnostic-and-lightweight","title":"Framework-agnostic and lightweight","text":"<p>Tungstenkit doesn't restrict you to use specific ML libraries. Just use any library you want, and declare dependencies:</p> <pre><code># The latest cpu-only build of Tensorflow will be included\n@define_model(gpu=False, python_packages=[\"tensorflow\"])\nclass TensorflowModel:\ndef predict(self, inputs):\n\"\"\"Run a batch prediction\"\"\"\n# ...ops using tensorflow...\nreturn outputs\n</code></pre>"},{"location":"#pydantic-inputoutput-definitions-with-convenient-file-handling","title":"Pydantic input/output definitions with convenient file handling","text":"<p>Let's look at the example below: <pre><code>from tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nimage: Image\n@define_model(input=Input, output=Output)\nclass StyleTransferModel:\n...\n</code></pre> As you see, input/output types are defined as subclasses of the <code>BaseIO</code> class. The <code>BaseIO</code> class is a simple wrapper of the <code>BaseModel</code> class of Pydantic, and Tungstenkit validates JSON requests utilizing functionalities Pydantic provides.</p> <p>Also, you can see that the <code>Image</code> class is used. Tungstenkit provides four file classes for easing file handling - <code>Image</code>, <code>Audio</code>, <code>Video</code>, and <code>Binary</code>. They have useful methods for writing a model's <code>predict</code> method:</p> <pre><code>class StyleTransferModel:\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n# Preprocessing\ninput_pil_images = [inp.image.to_pil_image() for inp in inputs]\n# Inference\noutput_pil_images = do_inference(input_pil_images)\n# Postprocessing\noutput_images = [Image.from_pil_image(pil_image) for pil_image in output_pil_images]\noutputs = [Output(image=image) for image in output_images]\nreturn outputs\n</code></pre>"},{"location":"#supports-batched-prediction","title":"Supports batched prediction","text":"<p>Tungstenkit supports both server-side and client-side batching.</p> <ul> <li> <p>Server-side batching      A server groups inputs across multiple requests and processes them together.     You can configure the max batch size:     <pre><code>@define_model(input=Input, output=Output, gpu=True, batch_size=32)\n</code></pre>     The max batch size can be changed when running a server:     <pre><code>$ tungsten serve mymodel -p 3000 --batch-size 16\n</code></pre></p> </li> <li> <p>Client-side batching     Also, you can reduce traffic volume by putting multiple inputs in a single prediction request:     <pre><code>$ curl -X 'POST' 'http://localhost:3000/predictions' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"field\": \"input1\"}, {\"field\": \"input2\"}, {\"field\": \"input3\"}]'\n</code></pre></p> </li> </ul>"},{"location":"#join-our-community","title":"Join our community","text":"<p>If you have questions about anything related to Tungstenkit, you're always welcome to ask our community on Discord.</p>"},{"location":"cli_reference/","title":"CLI Reference","text":""},{"location":"cli_reference/#global-options","title":"Global options","text":"<ul> <li><code>--debug</code>: Show logs for debugging.</li> <li><code>--help (-h)</code>: Show the help message and exit.</li> </ul>"},{"location":"cli_reference/#build","title":"<code>build</code>","text":"<p>This command is for containerizing your model.</p> <p><pre><code>tungsten build .\n</code></pre> will create a docker image with <code>tungsten_model.py</code> in the current directory.</p>"},{"location":"cli_reference/#options","title":"Options","text":"<ul> <li><code>--name (-n)</code>: Name of the model in <code>&lt;repo name&gt;[:&lt;tag&gt;]</code> format<ul> <li>Default: <code>BUILD_DIRECTORY_NAME:MODEL_ID</code> (e.g. <code>current-directory:41ea3bf</code>)</li> </ul> </li> <li><code>--model-module (-m)</code>: Model module (e.g., <code>some.example.module</code>)<ul> <li>Default: <code>tungsten_model</code></li> </ul> </li> <li><code>--model-class (-c)</code>: Model class (e.g., <code>MyModel</code>)  <ul> <li>Default: the class decorated with <code>tungstenkit.define_model</code></li> </ul> </li> <li><code>--copy-files</code>: Copy files to the container in <code>&lt;src in host&gt;:&lt;dest in container&gt;</code> format</li> </ul>"},{"location":"cli_reference/#models","title":"<code>models</code>","text":"<p>The <code>models</code> command displays all the available models.</p> <pre><code>tungsten models\n</code></pre>"},{"location":"cli_reference/#serve","title":"<code>serve</code>","text":"<p>The <code>serve</code> command runs a REST API server with a model. <pre><code>tungsten serve mymodel:v1\n</code></pre></p> <p>If you want to run with the latest model in a repository, you can omit the tag. <pre><code>tungsten serve mymodel\n</code></pre></p> <p>If you want to run with the latest model, you can omit the model name. <pre><code>tungsten serve\n</code></pre></p>"},{"location":"cli_reference/#options_1","title":"Options","text":"<ul> <li><code>--port (-p)</code>: Bind socket to this port.<ul> <li>Default: <code>3000</code></li> </ul> </li> <li><code>--batch-size</code>: Max batch size for adaptive batching.<ul> <li>Default: Declared value in <code>tungsten_model.py</code></li> </ul> </li> <li><code>--log-level</code>: Log level of the server.<ul> <li>Default: <code>info</code></li> <li>Available values: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code></li> </ul> </li> </ul>"},{"location":"cli_reference/#demo","title":"<code>demo</code>","text":"<p>The <code>demo</code> command runs an interactive web demo with a model. <pre><code>tungsten demo mymodel:v1\n</code></pre></p> <p>If you want to run with the latest model in a repository, you can omit the tag. <pre><code>tungsten demo mymodel\n</code></pre></p> <p>If you want to run with the latest model, you can omit the model name. <pre><code>tungsten demo\n</code></pre></p>"},{"location":"cli_reference/#options_2","title":"Options","text":"<ul> <li><code>--port (-p)</code>: The port on which the demo server will listen.<ul> <li>Default: <code>3300</code></li> </ul> </li> <li><code>--host</code>: The host on which the demo server will listen<ul> <li>Default: <code>localhost</code></li> </ul> </li> <li><code>--batch-size</code>: Max batch size for adaptive batching.<ul> <li>Default: Declared value in <code>tungsten_model.py</code></li> </ul> </li> <li><code>--log-level</code>: Log level of the server.<ul> <li>Default: <code>info</code></li> <li>Available values: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code></li> </ul> </li> </ul>"},{"location":"cli_reference/#predict","title":"<code>predict</code>","text":"<p>This command is for running a prediction with a model.</p> <pre><code>tungsten predict stable-diffusion:1.5 -i prompt=\"astronaut\" -i seed=1234\n</code></pre>"},{"location":"cli_reference/#options_3","title":"Options","text":"<ul> <li><code>--input (-i)</code>: Input field in the format of <code>&lt;NAME&gt;=&lt;VALUE&gt;</code></li> <li><code>--output-file-dir</code>: Output file directory<ul> <li>Default: <code>.</code></li> </ul> </li> </ul>"},{"location":"cli_reference/#tag","title":"<code>tag</code>","text":"<p>The <code>tag</code> command adds a new name of a model.</p> <pre><code>tungsten tag mymodel:v1 mymodel:v1.1\n</code></pre>"},{"location":"cli_reference/#remove","title":"<code>remove</code>","text":"<p>The <code>remove</code> command removes a model.</p> <pre><code>tungsten remove mymodel:v1\n</code></pre>"},{"location":"cli_reference/#clear","title":"<code>clear</code>","text":"<p>The <code>clear</code> command removes all models.</p> <pre><code>tungsten clear\n</code></pre> <p>If you want to remove all models in a repository (e.g. all models whose name starts with <code>mymodel</code>), you can put the repository name.</p> <pre><code>tungsten clear mymodel\n</code></pre>"},{"location":"cli_reference/#login","title":"<code>login</code>","text":"<p>This command is for logging in to tungsten.run.</p> <pre><code>tungsten login\n</code></pre>"},{"location":"cli_reference/#push","title":"<code>push</code>","text":"<p>This command is for pushing a model to tungsten.run. Before running this command, you should login to tungsten.run using <code>login</code> command.</p> <pre><code>tungsten push exampleuser/exampleproject:exampleversion\n</code></pre> <p>If you already logged in with the username of <code>exampleuser</code>, the above command is equivalent to:</p> <pre><code>tungsten push exampleproject:exampleversion\n</code></pre> <p>If you want to push the latest model, you can omit the model name. <pre><code>tungsten push\n</code></pre></p>"},{"location":"cli_reference/#pull","title":"<code>pull</code>","text":"<p>This command is for pulling a model from tungsten.run.</p> <pre><code>tungsten pull exampleuser/exampleproject:exampleversion\n</code></pre>"},{"location":"rest_api_reference/","title":"REST API Reference","text":""},{"location":"rest_api_reference/#get","title":"<code>GET /</code>","text":"<p>Get input/output JSON schemas of this model. </p> <p>The response is a JSON object with the following fields:</p> <ul> <li><code>input_schema</code>: Input JSON schema. Keys are same as the input class declared using <code>define_model</code> decorator.</li> <li><code>output_schema</code>: Output JSON schema. Keys are same as the output class declared using <code>define_model</code> decorator. </li> <li><code>demo_output_schema</code>: Demo output JSON schema. Keys are same as the demo output class declared using <code>define_model</code> decorator.</li> </ul>"},{"location":"rest_api_reference/#post-predictions","title":"<code>POST /predictions</code>","text":"<p>Create a prediction. </p> <p>The request body should be a list of input JSON objects.  </p> <p>The response is a JSON object with the <code>prediction_id</code> field.</p> <p>For example, let's assume that we defined the input as follows:</p> <pre><code>from tungstenkit import BaseIO, define_model\nclass Input(BaseIO):\nprompt: str\nseed: int\n@define_model(input=Input, ...)\nclass TextToImageModel:\n...\n</code></pre> <p>Then, you can send a request as follows:</p> <pre><code>$ curl -X 'POST' 'http://localhost:3000/predictions' \\\n-H 'Accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"prompt\": \"a professional photograph of an astronaut riding a horse\"}]'\n{\n    \"prediction_id\": \"39c9eb6b\"\n}\n</code></pre>"},{"location":"rest_api_reference/#get-predictionsprediction_id","title":"<code>GET /predictions/{prediction_id}</code>","text":"<p>Get the result of an asynchronous prediction.</p> <p>The response is a JSON object with the following fields:</p> <ul> <li><code>outputs</code>: List of output JSON objects. </li> <li><code>status</code>: Among <code>pending</code>, <code>running</code>, <code>success</code> and <code>failed</code>.</li> <li><code>error_message</code>: The error message if <code>status</code> is <code>failed</code>.</li> </ul> <p>For example, let's assume that we defined the output as follows:</p> <pre><code>from tungstenkit import BaseIO, define_model, Image\nclass Output(BaseIO):\nimage: Image\n@define_model(output=Output, ...)\nclass TextToImageModel:\n...\n</code></pre> <p>Then, you can send a request as follows:</p> <pre><code>$ curl -X 'GET' 'http://localhost:3000/predictions/39c9eb6b' \\\n-H 'Accept: application/json'\n{\n    \"outputs\": [{\"image\": \"data:image/png;base64,...\"}],\n    \"status\": \"success\"\n}\n</code></pre>"},{"location":"rest_api_reference/#post-predictionsprediction_idcancel","title":"<code>POST /predictions/{prediction_id}/cancel</code>","text":"<p>Cancel an asynchronous prediction.</p>"},{"location":"rest_api_reference/#post-demo","title":"<code>POST /demo</code>","text":"<p>Make a demo prediction. </p> <p>The request body should be a list of input JSON objects.  </p> <p>The response is a JSON object with the <code>demo_id</code> field.</p>"},{"location":"rest_api_reference/#get-demodemo_id","title":"<code>GET /demo/{demo_id}</code>","text":"<p>Get the result of a demo prediction.</p> <p>The response is a JSON object with the following fields:</p> <ul> <li><code>outputs</code>: List of output JSON objects. </li> <li><code>demo_outputs</code>: List of demo output JSON objects.</li> <li><code>status</code>: Among <code>pending</code>, <code>running</code>, <code>success</code> and <code>failed</code>.</li> <li><code>error_message</code>: The error message if <code>status</code> if <code>failed</code>.</li> <li><code>logs</code>: Logs while running the <code>predict_demo</code> method.</li> </ul>"},{"location":"rest_api_reference/#post-demodemo_idcancel","title":"<code>POST /demo/{demo_id}/cancel</code>","text":"<p>Cancel a demo prediction.</p>"},{"location":"building_your_model/containerizing/","title":"Containerizing","text":"<p>With <code>tungsten_model.py</code>, you can containerize a model using <code>tungsten build</code> command:</p> <pre><code>$ tungsten build . -n mymodel\n\n\u2705 Successfully built tungsten model: 'mymodel:e3a5de56'\n</code></pre> <p>Then you can see that this model has been added to the model list: <pre><code>$ tungsten models\n\nRepository        Tag       Create Time          Docker Image ID\n----------------  --------  -------------------  ---------------\nmymodel           e3a5de56    2023-04-26 05:23:58  830eb82f0fcd\n</code></pre></p>"},{"location":"building_your_model/gpu_models/","title":"GPU Models","text":""},{"location":"building_your_model/gpu_models/#declare-as-a-gpu-model","title":"Declare as a GPU model","text":"<p>You can set <code>gpu=True</code> in the <code>define_model</code> decorator:</p> <p><pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import BaseIO, Field, Image, define_model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str = Field(choices=LABELS)\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass ImageClassifier:\ndef setup(self):\n\"\"\"Load the model into memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre> Then, Tungstenkit automatically selects a compatible CUDA version and installs it in the container if required. The CUDA version inference is currently supported on <code>torch</code>, <code>torchvision</code>, <code>torchaudio</code>, and <code>tensorflow</code>.</p>"},{"location":"building_your_model/gpu_models/#declare-gpu-memory-size","title":"Declare GPU memory size","text":"<p>You can define the minimum GPU memory size required to run this model. </p> <pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import BaseIO, Field, Image, define_model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str = Field(choices=LABELS)\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\ngpu_mem_gb=6,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass ImageClassifier:\ndef setup(self):\n\"\"\"Load the model into memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre> <p>Then, the VM type is automatically determined when you push the model to tungsten.run. By default, the minimum GPU memory size is set as 16GB.</p>"},{"location":"building_your_model/gpu_models/#manually-set-the-cuda-version","title":"Manually set the CUDA version","text":"<p>You can also pass <code>cuda_version</code> as an argument of the <code>tungstenkit.define_model</code> decorator:</p> <pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import BaseIO, Field, Image, define_model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str = Field(choices=LABELS)\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\ncuda_version=\"11.8\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass ImageClassifier:\ndef setup(self):\n\"\"\"Load the model into memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre>"},{"location":"building_your_model/gpu_models/#force-to-install-system-cuda","title":"Force to install system CUDA","text":"<p>By default, Tungstenkit doesn't install system CUDA unless a known GPU package requires it. But in some cases, you may want to change this behavior. For this, you can use <code>force_install_system_cuda</code> flag of the <code>tungstenkit.define_model</code> decorator.</p> <pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import BaseIO, Field, Image, define_model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str = Field(choices=LABELS)\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\ncuda_version=\"11.8\",\nforce_install_system_cuda=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass ImageClassifier:\ndef setup(self):\n\"\"\"Load the model into memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre>"},{"location":"building_your_model/input_and_output/","title":"Input and Output","text":"<p>You can define input and output for image classification like this:</p> <pre><code>from tungstenkit import BaseIO, Image\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n</code></pre> <p>Here you see:</p> <ul> <li>Input and output classes are inherited from <code>tungstenkit.BaseIO</code>.</li> <li>Input has a field named 'image', and its type is <code>tungstenkit.Image</code>.</li> <li>Output has two fields named 'score' and 'label', and their types are <code>float</code> and <code>str</code>.</li> </ul>"},{"location":"building_your_model/input_and_output/#the-baseio-class","title":"The <code>BaseIO</code> class","text":"<p>The <code>BaseIO</code> class is the base class for all inputs and outputs in Tungstenkit. </p> <p>It is a simple wrapper of the <code>BaseModel</code> class in Pydantic, so you can use all useful methods and attributes it provides.</p>"},{"location":"building_your_model/input_and_output/#supported-field-types","title":"Supported field types","text":"<p>Tungstenkit currently supports the following input/output field types:</p> Type Input Output <code>tungstenkit.Image</code> \u2705 \u2705 <code>tungstenkit.Video</code> \u2705 \u2705 <code>tungstenkit.Audio</code> \u2705 \u2705 <code>tungstenkit.Binary</code> \u2705 \u2705 <code>str</code> \u2705 \u2705 <code>float</code> \u2705 \u2705 <code>int</code> \u2705 \u2705 <code>bool</code> \u2705 \u2705 <code>dict</code> or <code>typing.Dict</code> \u274c \u2705 <code>list</code> or <code>typing.List</code> \u274c \u2705 <code>tuple</code> or <code>typing.Tuple</code> \u274c \u2705 A subclass of <code>tungstenkit.BaseIO</code> \u274c \u2705 <p>For <code>dict</code>, <code>list</code>, <code>tuple</code>, <code>typing.Dict</code>, <code>typing.List</code> and <code>typing.Tuple</code>, type arguments are required. For example, you should use <code>dict[str, str]</code> instead of <code>dict</code>.</p>"},{"location":"building_your_model/input_and_output/#files","title":"Files","text":"<p>Tungstenkit provides four primitives for files: <code>Image</code>, <code>Video</code>, <code>Audio</code>, and <code>Binary</code>. They possess the following property and method:</p> <ul> <li><code>path</code> : a string of the file path.</li> <li><code>from_path(path: StrPath)</code>: a class method for creating file objects from a filepath. <pre><code>&gt;&gt;&gt; from tungstenkit import Video\n&gt;&gt;&gt; video_path = \"video.mp4\"\n&gt;&gt;&gt; video = Video.from_path(video_path)\n&gt;&gt;&gt; video.path\n'/home/tungsten/working_dir/video.mp4'\n</code></pre></li> </ul> <p>The <code>Image</code> object has more methods:</p> <ul> <li><code>from_pil_image(pil_image: PIL.Image.Image)</code>: a class method for creating the object from a <code>PIL.Image.Image</code> object.</li> <li><code>to_pil_image(mode: str = \"RGB\")</code>: returns a <code>PIL.Image.Image</code> object.</li> </ul>"},{"location":"building_your_model/input_and_output/#input-field-descriptors","title":"Input field descriptors","text":"<p>Tungstenkit contains two input field descriptors: <code>Field</code> and <code>Option</code> functions:</p> <ul> <li><code>Field</code>: For setting properties of a required field.</li> <li><code>Option</code>: For declaring a field as optional and setting its properties. Optional fields will be same in an input batch and hidden in the model demo page by default.</li> </ul> <p>Using them, you can:</p> <ul> <li>Distinguish between required and optional fields.</li> <li>Restrict input field values.</li> <li>Set input field descriptions shown in the model demo page.</li> </ul> <p>For example, you can define an input class for text-to-image generation as follows: <pre><code>from tungstenkit import BaseIO, Field, Option\nclass Input(BaseIO):\nprompt: str = Field(\ndescription=\"Input prompt\", \nmin_length=1, \nmax_length=200\n)\nwidth: int = Option(\ndescription=\"Width of output image\",\nchoices=[128, 256, 512],\ndefault=512,\n)\nheight: int = Option(\ndescription=\"Height of output image\",\nchoices=[128, 256, 512],\ndefault=512,\n)\n</code></pre></p> <p>Both field descriptors take the following keyword-only arguments:</p> <ul> <li><code>description</code> (<code>str</code>, optional): Human-readable description.</li> <li><code>ge</code> (<code>float</code>, optional): Greater than or equal. If set, value must be greater than or equal to this. Only applicable to <code>int</code> and <code>float</code>.</li> <li><code>le</code> (<code>float</code>, optional): Less than or equal. If set, value must be less than or equal to this. Only applicable to <code>int</code> and <code>float</code>.</li> <li><code>min_length</code> (<code>int</code>, optional): Minimum length for strings.</li> <li><code>max_length</code> (<code>int</code>, optional): Maximum length for strings.</li> <li><code>choices</code> (<code>list</code>, optional): List of possible values. If set, value must be among a value in this.</li> </ul> <p>The <code>Option</code> function additionally takes a positional argument:</p> <ul> <li><code>default</code> (<code>Any</code>, required): Default value.</li> </ul>"},{"location":"building_your_model/model_definition/","title":"Model Definition","text":"<p>A Tungsten model is defined as a class with <code>define_model</code> decorator in <code>tungsten_model.py</code>:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nprompt: str\nclass Output(BaseIO):\nimage: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ngpu_mem_gb=16,\n)\nclass TextToImageModel:\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> <p>Here you can find the elements needed to define a Tungsten model:</p> <ul> <li>Input/Output classes</li> <li><code>setup</code> method</li> <li><code>predict</code> method</li> <li>Runtime configuration (e.g. batch size &amp; device type)</li> <li>Dependencies (e.g. Python packages)</li> </ul>"},{"location":"building_your_model/model_definition/#basic-usage","title":"Basic Usage","text":""},{"location":"building_your_model/model_definition/#declare-inputoutput-types","title":"Declare input/output types","text":"<p>Input/output types are declared by passing them as arguments of <code>define_model</code> decorator:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nprompt: str\nclass Output(BaseIO):\nimage: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ngpu_mem_gb=16,\n)\nclass TextToImageModel:\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> <p>See Building Your Model - Input/Output to learn how to define input/output.</p>"},{"location":"building_your_model/model_definition/#define-how-to-load-a-model","title":"Define how to load a model","text":"<p>You can define the <code>setup</code> method for loading a model:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nprompt: str\nclass Output(BaseIO):\nimage: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ngpu_mem_gb=16,\n)\nclass TextToImageModel:\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> <p>As you can see, the <code>weights.pth</code> file is required to setup. Before containerizing, you should make sure that the file exists in the build directory.</p>"},{"location":"building_your_model/model_definition/#define-how-a-prediction-works","title":"Define how a prediction works","text":"<p>The <code>predict</code> method defines the computation performed at every prediction request.</p> <p>It takes a non-empty list of <code>Input</code> objects as an argument, and should return a list of the same number of <code>Output</code> objects.</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nprompt: str\nclass Output(BaseIO):\nimage: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ngpu_mem_gb=16,\n)\nclass TextToImageModel:\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> <p>During runtime, Tungstenkit automatically keeps all options same in a batch. So, you can define the <code>predict</code> method of a text-to-image generaton model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, define_model, Option\nclass Input(BaseIO):\nprompt: str\nwidth: int = Option(\nchoices=[128, 256],\ndefault=256,\n)\nheight: int = Option(\nchoices=[128, 256],\ndefault=256,\n)\nclass Output(BaseIO):\nimage: Image\n@define_model(input=Input, output=Output)\nclass TextToImageModel:\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\noptions = inputs[0]\nprompts = [inp.prompt for inp in inputs]\npil_images = self.model(prompts, width=options.width, height=options.height)\nimages = [Image.from_pil_image(pil_image) for pil_image in pil_images]\noutputs = [Output(image=image) for image in images]\nreturn outputs\n</code></pre>"},{"location":"building_your_model/model_definition/#declaring-dependencies-and-runtime-configuration","title":"Declaring dependencies and runtime configuration","text":"<p>You can declare dependencies and runtime configuration via <code>define_model</code> decorator: <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nprompt: str\nclass Output(BaseIO):\nimage: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ngpu_mem_gb=16,\n)\nclass TextToImageModel:\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre></p> <p>The <code>define_model</code> decorator takes the following keyword-only arguments:</p> <ul> <li><code>input (BaseIO)</code>: The input class.</li> <li><code>output (BaseIO)</code>: The output class.</li> <li><code>demo_output (BaseIO | None)</code>: The demo output class. It is required only when the <code>predict_demo</code> method is defined (default: <code>None</code>).</li> <li><code>batch_size (int)</code>: Max batch size for adaptive batching (default: <code>1</code>).</li> <li><code>gpu (bool)</code>: Indicates if the model requires GPUs (default: <code>False</code>).</li> <li><code>cuda_version (str | None)</code>: CUDA version in <code>&lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;]]</code> format. If <code>None</code> (default), the cuda version will be automatically determined as compatible with <code>python_packages</code>. Otherwise, fix the CUDA version as <code>cuda_version</code>.</li> <li><code>gpu_mem_gb (int)</code>: Minimum GPU memory size required to run the model (default: <code>16</code>). This argument will be ignored if <code>gpu==False</code>.</li> <li><code>python_packages (list[str] | None)</code>: A list of pip requirements in <code>&lt;name&gt;[==&lt;version&gt;]</code> format. If <code>None</code> (default), no extra Python packages are added.</li> <li><code>python_version (str | None)</code>: Python version to use in <code>&lt;major&gt;[.&lt;minor&gt;]</code> format. If <code>None</code> (default), the python version will be automatically determined as compatible with <code>python_packages</code> while prefering the current Python version. Otherwise, fix the Python version as <code>python_version</code>.</li> <li><code>system_packages (list[str] | None)</code>: A list of system packages that will installed by the system package manager (e.g. <code>apt</code>). The default value is <code>None</code>. This argument will be ignored while using a custom base image, because Tungstenkit cannot decide which package manager to use.</li> <li><code>mem_gb (int)</code>: Minimum memory size required to run the model (default: <code>8</code>).</li> <li><code>include_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code>. If <code>None</code> (default), all files in the working directory and its subdirectories are added, which is equivalent to <code>[*]</code>.</li> <li><code>exclude_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code> for matching which files to exclude. If <code>None</code> (default), all hidden files and Python bytecodes are ignored, which is equivalent to <code>[\".*/\", \"__pycache__/\", \"*.pyc\", \"*.pyo\", \"*.pyd\"]</code>.</li> <li><code>dockerfile_commands (list[str] | None)</code>: A list of dockerfile commands (default: <code>None</code>). The commands will be executed before setting up python packages.</li> <li><code>base_image (str | None)</code>: Base docker image in <code>&lt;repository&gt;[:&lt;tag&gt;]</code> format. If <code>None</code> (default), the base image is automatically selected with respect to pip packages, the device type, and the CUDA version. Otherwise, use it as the base image and <code>system_packages</code> will be ignored.</li> </ul>"},{"location":"building_your_model/model_definition/#advanced-usage","title":"Advanced Usage","text":""},{"location":"building_your_model/model_definition/#define-how-a-demo-prediction-works","title":"Define how a demo prediction works","text":"<p>You can define an object detection model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, define_model\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\n@define_model(input=Input, output=Output)\nclass ObjectDetectionModel:\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>But it doesn't contain any visualization, so you'll only get raw JSONs on the demo page.</p> <p>Then, you could add the visualization result to the output:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, define_model\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nvisualized: Image\n@define_model(input=Input, output=Output)\nclass ObjectDetectionModel:\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>This can improve the demo page, but introduces the visualization overhead of the API.</p> <p>In such a case, you can separate the method for demo predictions:</p> <p><pre><code>from typing import List, Tuple, Dict\nfrom tungstenkit import BaseIO, Image, define_model\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nclass Visualization(BaseIO):\nresult: Image\n@define_model(\ninput=Input, \noutput=Output,\ndemo_output=Visualization,\n)\nclass ObjectDetectionModel:\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\ndef predict_demo(\nself, \ninputs: List[Input]\n) -&gt; Tuple[List[Output], List[Visualization]]:\noutputs = self.predict(inputs)\npil_images = visualize(inputs, outputs)\nimages = [Image.from_pil_image(pil_image) for pil_image in pil_images]\nvisualizations = [Visualization(result=image) for image in images]\nreturn outputs, visualizations\n</code></pre> Then, the <code>predict</code> method is executed when a prediction is requested through the API, but the <code>predict_demo</code> method is called for a demo request.</p>"},{"location":"building_your_model/predict_function/","title":"Predict function","text":"<p>A Tungsten model is defined as a class using <code>tungstenkit.define_model</code> decorator in <code>tungsten_model.py</code>:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre>"},{"location":"building_your_model/predict_function/#basic-usage","title":"Basic Usage","text":""},{"location":"building_your_model/predict_function/#declare-inputoutput-types","title":"Declare input/output types","text":"<p>Input/output types are declared by passing them as type arguments to <code>TungstenModel</code> class:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>See Tungsten Model - Input/Output to learn how to define input/output.</p>"},{"location":"building_your_model/predict_function/#define-how-to-load-a-model","title":"Define how to load a model","text":"<p>You can override the <code>setup</code> method to define how to load a model:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>As you can see, the <code>weights.pth</code> file is required to setup. Before building, you should make sure that the file exists in the build directory.</p>"},{"location":"building_your_model/predict_function/#define-how-a-prediction-works","title":"Define how a prediction works","text":"<p>The <code>predict</code> method defines the computation performed at every prediction request.</p> <p>It takes a non-empty list of <code>Input</code> objects as an argument, and should return a list of the same number of <code>Output</code> objects.</p> <p>It should be overridden by all subclasses:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>During runtime, Tungstenkit automatically keeps all options same in a batch. So, you can define the <code>predict</code> method of a text-to-image generaton model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, TungstenModel, model_config, Option\nclass Input(BaseIO):\nprompt: str\nwidth: int = Option(\nchoices=[128, 256],\ndefault=256,\n)\nheight: int = Option(\nchoices=[128, 256],\ndefault=256,\n)\nclass Output(BaseIO):\nimage: Image\nclass Model(TungstenModel[Input, Output]):\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\noptions = inputs[0]\nprompts = [inp.prompt for inp in inputs]\npil_images = self.model(prompts, width=options.width, height=options.height)\nimages = [Image.from_pil_image(pil_image) for pil_image in pil_images]\noutputs = [Output(image=image) for image in images]\nreturn outputs\n</code></pre>"},{"location":"building_your_model/predict_function/#add-dependencies-and-explanations","title":"Add dependencies and explanations","text":"<p>You can add dependencies and explanations via the <code>config</code> decorator: <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre></p> <p>The <code>config</code> decorator takes the following keyword-only arguments:</p> <ul> <li><code>description (str | None)</code>: A text explaining the model (default: <code>None</code>).</li> <li><code>readme_md (str | None)</code>: Path to the <code>README.md</code> file (default: <code>None</code>).</li> <li><code>batch_size (int)</code>: Max batch size for adaptive batching (default: <code>1</code>).</li> <li><code>gpu (bool)</code>: Indicates if the model requires GPUs (default: <code>False</code>).</li> <li><code>cuda_version (str | None)</code>: CUDA version in <code>&lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;]]</code> format. If <code>None</code> (default), the cuda version will be automatically determined as compatible with <code>python_packages</code>. Otherwise, fix the CUDA version as <code>cuda_version</code>.</li> <li><code>gpu_mem_gb (int)</code>: Minimum GPU memory size required to run the model (default: <code>16</code>). This argument will be ignored if <code>gpu==False</code>.</li> <li><code>python_packages (list[str] | None)</code>: A list of pip requirements in <code>&lt;name&gt;[==&lt;version&gt;]</code> format. If <code>None</code> (default), no extra Python packages are added.</li> <li><code>python_version (str | None)</code>: Python version to use in <code>&lt;major&gt;[.&lt;minor&gt;]</code> format. If <code>None</code> (default), the python version will be automatically determined as compatible with <code>python_packages</code> while prefering the current Python version. Otherwise, fix the Python version as <code>python_version</code>.</li> <li><code>system_packages (list[str] | None)</code>: A list of system packages that will installed by the system package manager (e.g. <code>apt</code>). The default value is <code>None</code>. This argument will be ignored while using a custom base image, because Tungstenkit cannot decide which package manager to use.</li> <li><code>mem_gb (int)</code>: Minimum memory size required to run the model (default: <code>8</code>).</li> <li><code>include_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code>. If <code>None</code> (default), all files in the working directory and its subdirectories are added, which is equivalent to <code>[*]</code>.</li> <li><code>exclude_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code> for matching which files to exclude. If <code>None</code> (default), all hidden files and Python bytecodes are ignored, which is equivalent to <code>[\".*/\", \"__pycache__/\", \"*.pyc\", \"*.pyo\", \"*.pyd\"]</code>.</li> <li><code>dockerfile_commands (list[str] | None)</code>: A list of dockerfile commands (default: <code>None</code>). The commands will be executed before setting up python packages.</li> <li><code>base_image (str | None)</code>: Base docker image in <code>&lt;repository&gt;[:&lt;tag&gt;]</code> format. If <code>None</code> (default), the base image is automatically selected with respect to pip packages, the device type, and the CUDA version. Otherwise, use it as the base image and <code>system_packages</code> will be ignored.</li> </ul>"},{"location":"building_your_model/predict_function/#advanced-usage","title":"Advanced Usage","text":""},{"location":"building_your_model/predict_function/#define-how-a-demo-prediction-works","title":"Define how a demo prediction works","text":"<p>You can define an object detection model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, TungstenModel\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>But it doesn't contain any visualization, so you'll only get raw JSONs on the demo page.</p> <p>Then, you could add the visualization result to the output:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, TungstenModel\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nvisualized: Image\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>This can improve the demo page, but introduces the visualization overhead of the API.</p> <p>In such a case, you can separate the method for demo predictions:</p> <p><pre><code>from typing import List, Tuple, Dict\nfrom tungstenkit import BaseIO, Image, TungstenModel\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nclass Visualization(BaseIO):\nresult: Image\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\ndef predict_demo(\nself, \ninputs: List[Input]\n) -&gt; Tuple[List[Output], List[Visualization]]:\noutputs = self.predict(inputs)\npil_images = visualize(inputs, outputs)\nimages = [Image.from_pil_image(pil_image) for pil_image in pil_images]\nvisualizations = [Visualization(result=image) for image in images]\nreturn outputs, visualizations\n</code></pre> Then, the <code>predict</code> method is executed when a prediction is requested through the API, but the <code>predict_demo</code> method is called for a demo request.</p>"},{"location":"building_your_model/setup_function/","title":"Setup function","text":"<p>A Tungsten model is defined as a class using <code>tungstenkit.define_model</code> decorator in <code>tungsten_model.py</code>:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre>"},{"location":"building_your_model/setup_function/#basic-usage","title":"Basic Usage","text":""},{"location":"building_your_model/setup_function/#declare-inputoutput-types","title":"Declare input/output types","text":"<p>Input/output types are declared by passing them as type arguments to <code>TungstenModel</code> class:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>See Tungsten Model - Input/Output to learn how to define input/output.</p>"},{"location":"building_your_model/setup_function/#define-how-to-load-a-model","title":"Define how to load a model","text":"<p>You can override the <code>setup</code> method to define how to load a model:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>As you can see, the <code>weights.pth</code> file is required to setup. Before building, you should make sure that the file exists in the build directory.</p>"},{"location":"building_your_model/setup_function/#define-how-a-prediction-works","title":"Define how a prediction works","text":"<p>The <code>predict</code> method defines the computation performed at every prediction request.</p> <p>It takes a non-empty list of <code>Input</code> objects as an argument, and should return a list of the same number of <code>Output</code> objects.</p> <p>It should be overridden by all subclasses:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>During runtime, Tungstenkit automatically keeps all options same in a batch. So, you can define the <code>predict</code> method of a text-to-image generaton model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, TungstenModel, model_config, Option\nclass Input(BaseIO):\nprompt: str\nwidth: int = Option(\nchoices=[128, 256],\ndefault=256,\n)\nheight: int = Option(\nchoices=[128, 256],\ndefault=256,\n)\nclass Output(BaseIO):\nimage: Image\nclass Model(TungstenModel[Input, Output]):\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\noptions = inputs[0]\nprompts = [inp.prompt for inp in inputs]\npil_images = self.model(prompts, width=options.width, height=options.height)\nimages = [Image.from_pil_image(pil_image) for pil_image in pil_images]\noutputs = [Output(image=image) for image in images]\nreturn outputs\n</code></pre>"},{"location":"building_your_model/setup_function/#add-dependencies-and-explanations","title":"Add dependencies and explanations","text":"<p>You can add dependencies and explanations via the <code>config</code> decorator: <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, TungstenModel, model_config\nclass Input(BaseIO):\n...\nclass Output(BaseIO):\n...\n@model_config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre></p> <p>The <code>config</code> decorator takes the following keyword-only arguments:</p> <ul> <li><code>description (str | None)</code>: A text explaining the model (default: <code>None</code>).</li> <li><code>readme_md (str | None)</code>: Path to the <code>README.md</code> file (default: <code>None</code>).</li> <li><code>batch_size (int)</code>: Max batch size for adaptive batching (default: <code>1</code>).</li> <li><code>gpu (bool)</code>: Indicates if the model requires GPUs (default: <code>False</code>).</li> <li><code>cuda_version (str | None)</code>: CUDA version in <code>&lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;]]</code> format. If <code>None</code> (default), the cuda version will be automatically determined as compatible with <code>python_packages</code>. Otherwise, fix the CUDA version as <code>cuda_version</code>.</li> <li><code>gpu_mem_gb (int)</code>: Minimum GPU memory size required to run the model (default: <code>16</code>). This argument will be ignored if <code>gpu==False</code>.</li> <li><code>python_packages (list[str] | None)</code>: A list of pip requirements in <code>&lt;name&gt;[==&lt;version&gt;]</code> format. If <code>None</code> (default), no extra Python packages are added.</li> <li><code>python_version (str | None)</code>: Python version to use in <code>&lt;major&gt;[.&lt;minor&gt;]</code> format. If <code>None</code> (default), the python version will be automatically determined as compatible with <code>python_packages</code> while prefering the current Python version. Otherwise, fix the Python version as <code>python_version</code>.</li> <li><code>system_packages (list[str] | None)</code>: A list of system packages that will installed by the system package manager (e.g. <code>apt</code>). The default value is <code>None</code>. This argument will be ignored while using a custom base image, because Tungstenkit cannot decide which package manager to use.</li> <li><code>mem_gb (int)</code>: Minimum memory size required to run the model (default: <code>8</code>).</li> <li><code>include_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code>. If <code>None</code> (default), all files in the working directory and its subdirectories are added, which is equivalent to <code>[*]</code>.</li> <li><code>exclude_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code> for matching which files to exclude. If <code>None</code> (default), all hidden files and Python bytecodes are ignored, which is equivalent to <code>[\".*/\", \"__pycache__/\", \"*.pyc\", \"*.pyo\", \"*.pyd\"]</code>.</li> <li><code>dockerfile_commands (list[str] | None)</code>: A list of dockerfile commands (default: <code>None</code>). The commands will be executed before setting up python packages.</li> <li><code>base_image (str | None)</code>: Base docker image in <code>&lt;repository&gt;[:&lt;tag&gt;]</code> format. If <code>None</code> (default), the base image is automatically selected with respect to pip packages, the device type, and the CUDA version. Otherwise, use it as the base image and <code>system_packages</code> will be ignored.</li> </ul>"},{"location":"building_your_model/setup_function/#advanced-usage","title":"Advanced Usage","text":""},{"location":"building_your_model/setup_function/#define-how-a-demo-prediction-works","title":"Define how a demo prediction works","text":"<p>You can define an object detection model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, TungstenModel\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>But it doesn't contain any visualization, so you'll only get raw JSONs on the demo page.</p> <p>Then, you could add the visualization result to the output:</p> <pre><code>from typing import List\nfrom tungstenkit import BaseIO, Image, TungstenModel\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nvisualized: Image\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>This can improve the demo page, but introduces the visualization overhead of the API.</p> <p>In such a case, you can separate the method for demo predictions:</p> <p><pre><code>from typing import List, Tuple, Dict\nfrom tungstenkit import BaseIO, Image, TungstenModel\nclass BoundingBox(BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\ndetections: List[Detection]\nclass Visualization(BaseIO):\nresult: Image\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\ndef predict_demo(\nself, \ninputs: List[Input]\n) -&gt; Tuple[List[Output], List[Visualization]]:\noutputs = self.predict(inputs)\npil_images = visualize(inputs, outputs)\nimages = [Image.from_pil_image(pil_image) for pil_image in pil_images]\nvisualizations = [Visualization(result=image) for image in images]\nreturn outputs, visualizations\n</code></pre> Then, the <code>predict</code> method is executed when a prediction is requested through the API, but the <code>predict_demo</code> method is called for a demo request.</p>"},{"location":"building_your_model/using_custom_base_image/","title":"Using Custom Base Image","text":"<p>By default, Tungstenkit uses a Python or CUDA image as the base image for containerization. But, you can manually set a base image. </p> <p>You can use a custom base image by setting <code>base_image</code> in the <code>define_model</code> decorator: <pre><code>@define_model(\ninput=SDInput,\noutput=SDOutput,\nbatch_size=1,\ngpu=True,\ngpu_mem_gb=14,\nbase_image=\"mjpyeon/tungsten-sd-base:v1\",\n)\nclass StableDiffusion:\n...\n</code></pre></p> <p>In this case, since Tungstenkit doesn't know which OS the base image contains, <code>system_packages</code> argument of <code>define_model</code> decorator will be ignored.</p>"},{"location":"examples/image_blurring/","title":"Image Blurring","text":"<pre><code>from typing import List\nfrom PIL import ImageFilter\nfrom tungstenkit import BaseIO, Field, Image, define_model\nclass Input(BaseIO):\nimage: Image = Field(description=\"Image to blur\")\nclass Output(BaseIO):\nblurred: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=False,\npython_packages=[\"pillow\"],\n)\nclass ImageBlurringModel:\ndef setup(self):\nself.image_filter = ImageFilter.GaussianBlur(radius=5)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\nimages = [inp.image.to_pil_image() for inp in inputs]\nconverted = []\nfor img in images:\nconverted.append(img.filter(self.image_filter))\nreturn [Output(blurred=Image.from_pil_image(img)) for img in images]\n</code></pre>"},{"location":"examples/image_classification/","title":"Image Classification","text":"<pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import BaseIO, Field, Image, define_model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str = Field(choices=LABELS)\n@define_model(\ninput=Input,\noutput=Output,\ngpu=False,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass ImageClassificationModel:\ndef setup(self):\n\"\"\"Load the model into memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre>"},{"location":"examples/stable_diffusion/","title":"Stable Diffusion","text":"<p>The fastest way to build a Stable diffusion model is using Tungsten Stable Diffusion Template. See the repository for more details.</p>"},{"location":"getting_started/basic_example/","title":"Basic Example","text":""},{"location":"getting_started/basic_example/#create-a-directory","title":"Create a directory","text":"<p>Let's start by creating a working directory: <pre><code>mkdir tungsten-getting-started\ncd tungsten-getting-started\n</code></pre></p>"},{"location":"getting_started/basic_example/#write-tungsten_modelpy","title":"Write <code>tungsten_model.py</code>","text":"<p>You can write the <code>tungsten_model.py</code> file for an image classification model as follows: <pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import BaseIO, Field, Image, define_model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str = Field(choices=LABELS)\n@define_model(\ninput=Input,\noutput=Output,\ngpu=False,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass Model:\ndef setup(self):\n\"\"\"Load the model into memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre></p>"},{"location":"getting_started/basic_example/#download-the-required-files","title":"Download the required files","text":"<p>Before building, you should prepare the required files.</p> <p>As you can see above, two files are needed: <code>imagenet_labels.json</code> and <code>mobilenetv2_weights.pth</code>. Download these files via the script below: <pre><code>curl -o imagenet_labels.json -X GET https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json &amp;&amp; \\  curl -o mobilenetv2_weights.pth https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\n</code></pre></p>"},{"location":"getting_started/basic_example/#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Now everything is ready. Let's start building a Tungsten model: <pre><code>$ tungsten build -n tungsten-example\n\n\u2705 Successfully built tungsten model: 'tungsten-example:e3a5de5616a743fe9021e2dcfe1cd19a' (also tagged as 'tungsten-example:latest')\n</code></pre></p> <pre><code>$ tungsten models\n\nRepository        Tag                               Description                 Model Class           Created              Docker Image ID\n----------------  --------------------------------  --------------------------  --------------------  -------------------  -----------------\ntungsten-example  latest                            Image classification model  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\ntungsten-example  e3a5de5616a743fe9021e2dcfe1cd19a  Image classification model  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\n</code></pre>"},{"location":"getting_started/basic_example/#run-it","title":"Run it","text":"<p>Now, you can run the model in your local machine in multiple ways.</p>"},{"location":"getting_started/basic_example/#option-1-interactive-web-demo","title":"Option 1: Interactive web demo","text":"<p><pre><code>tungsten demo tungsten-example -p 8080\n</code></pre> Visit http://localhost:8080 to check:</p> <p></p>"},{"location":"getting_started/basic_example/#option-2-rest-api","title":"Option 2: REST API","text":"<p>Start the server: <pre><code>$ tungsten serve tungsten-example -p 3000\nINFO:     Setting up the model\nINFO:     Getting inputs from the input queue\nINFO:     Starting the prediction service\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre></p> <p>Send a prediction request with a JSON payload: <pre><code>$ curl -X 'POST' 'http://localhost:3000/predictions' \\\n-H 'Accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"image\": \"https://picsum.photos/200.jpg\"}]'\n{\n    \"prediction_id\": \"39c9eb6b\"\n}\n</code></pre></p> <p>Get the result: <pre><code>$ curl -X 'GET' 'http://localhost:3000/predictions/39c9eb6b' \\\n-H 'Accept: application/json'\n{\n    \"outputs\": [{\"scale\": 0.12483298, \"label\": \"dog\"}],\n    \"status\": \"success\"\n}\n</code></pre></p> <p>Also, you can find a Swagger documentation at http://localhost:3000/docs.</p>"},{"location":"getting_started/basic_example/#option-3-make-a-prediction-using-cli","title":"Option 3: Make a prediction (using CLI)","text":"<pre><code>$ tungsten predict tungsten-example -i image=\"https://picsum.photos/200.jpg\"\n{\n    \"scale\": 0.12483298, \"label\": \"dog\"\n}\n</code></pre>"},{"location":"getting_started/basic_example/#option-4-make-a-prediction-using-python","title":"Option 4: Make a prediction (using Python)","text":"<pre><code>&gt;&gt;&gt; from tungstenkit import models\n&gt;&gt;&gt; model = models.get(\"tungsten-example\")\n&gt;&gt;&gt; model.predict(\n{\"image\": \"https://picsum.photos/200.jpg\"}\n)\n{\"scale\": 0.12483298, \"label\": \"dog\"}\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>Docker</li> </ul>"},{"location":"getting_started/installation/#install-tungstenkit","title":"Install Tungstenkit","text":"<pre><code>pip install tungstenkit\n</code></pre>"},{"location":"getting_started/installation/#install-tungstenkit-from-source-code","title":"Install Tungstenkit from source code","text":"<pre><code>pip install git+https://github.com/tungsten-ai/tungstenkit\n</code></pre> <p>This command installs the latest <code>main</code> version of Tungstenkit.</p>"},{"location":"getting_started/project_structure/","title":"Project structure","text":"<p><code>tungsten_model.py</code> should exist in the project root directory. <code>tungsten_model.py</code> looks like:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import BaseIO, Image, define_model\nclass Input(BaseIO):\nprompt: str\nclass Output(BaseIO):\nimage: Image\n@define_model(\ninput=Input,\noutput=Output,\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ngpu_mem_gb=16,\n)\nclass TextToImageModel:\ndef setup(self):\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre>"},{"location":"pushing_and_pulling_models/pulling/","title":"Pulling","text":"<p>You can pull and use models from tungsten.run.</p> <pre><code>$ tungsten pull exampleuser/exampleproject:v1\n\n\u2705 Successfully pulled 'exampleuser/exampleproject:v1' from 'https://api.tungsten.run'\n</code></pre> <p>Then you can see that this model has been added to the model list: <pre><code>$ tungsten models\n\nRepository                     Tag   Create Time          Docker Image ID\n-----------------------------  ---   -------------------  ---------------\nexampleuser/exampleproject:v1  v1    2023-04-26 05:23:58  830eb82f0fcd\n</code></pre></p>"},{"location":"pushing_and_pulling_models/pushing/","title":"Pushing","text":"<p>You can store, share and run your models in tungsten.run.</p> <p>First, create a project in tungsten.run in a browser.</p> <p>Then, log in using Tungstenkit CLI to fetch the credential.</p> <pre><code>$ tungsten login\n\nUser (username or email): exampleuser\nPassword:\nLogin Success!\n</code></pre> <p>Tag a model as <code>&lt;USER_NAME&gt;/&lt;PROJECT_NAME&gt;:&lt;VERSION&gt;</code> format: <pre><code>$ tungsten tag mymodel:v1 exampleuser/exampleproject:v1\n\nTagged model 'mymodel:v1' to 'exampleuser/exampleproject:v1'.\n</code></pre></p> <p>Push it: <pre><code>$ tungsten push exampleuser/exampleproject:v1\n\n\u2705 Successfully pushed 'exampleuser/exampleproject:v1' to 'https://api.tungsten.run'\n  - project: exampleuser/exampleproject\n  - version: v1\n</code></pre></p>"},{"location":"running_models/demo/","title":"Serving Web Demo","text":"<p>After building or pulling a model, you can start an interactive web demo with it.</p> <p>Start the demo: <pre><code>$ tungsten demo text-to-image:v1 -p 8080\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n</code></pre></p> <p>Visit http://localhost:8080: </p> <p>If you want to run with the latest model in a repository, you can omit the tag: <pre><code>$ tungsten demo text-to-image\n</code></pre></p> <p>If you want to run with the latest model, you can omit the model name. <pre><code>$ tungsten demo\n</code></pre></p>"},{"location":"running_models/demo/#options","title":"Options","text":"<ul> <li><code>--port (-p)</code>: The port on which the demo server will listen.<ul> <li>Default: <code>3300</code></li> </ul> </li> <li><code>--host</code>: The host on which the demo server will listen<ul> <li>Default: <code>localhost</code></li> </ul> </li> <li><code>--batch-size</code>: Max batch size for adaptive batching.<ul> <li>Default: Declared value in <code>tungsten_model.py</code></li> </ul> </li> <li><code>--log-level</code>: Log level of the server.<ul> <li>Default: <code>info</code></li> <li>Available values: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code></li> </ul> </li> </ul>"},{"location":"running_models/making_a_single_prediction/","title":"Making Single Prediction","text":"<p>After building or pulling a model, you can make a prediction with it.</p>"},{"location":"running_models/making_a_single_prediction/#using-tungstenkit-cli","title":"Using Tungstenkit CLI","text":"<pre><code>$ tungsten predict text-to-image:v1 \\\n-i prompt=\"a professional photograph of an astronaut riding a horse\" \\\n-i seed=1234\n{\n  \"image\": \"./output.png\"\n}\n</code></pre> <p>If you want to run with the latest model in a repository, you can omit the tag: <pre><code>$ tungsten predict text-to-image \\\n-i prompt=\"a professional photograph of an astronaut riding a horse\" \\\n-i seed=1234\n</code></pre></p> <p>If you want to run with the latest model, you can omit the model name. <pre><code>$ tungsten predict \\\n-i prompt=\"a professional photograph of an astronaut riding a horse\" \\\n-i seed=1234\n</code></pre></p>"},{"location":"running_models/making_a_single_prediction/#options","title":"Options","text":"<ul> <li><code>--input (-i)</code>: Input field in the format of <code>&lt;NAME&gt;=&lt;VALUE&gt;</code></li> <li><code>--output-file-dir</code>: Output file directory<ul> <li>Default: <code>`.</code></li> </ul> </li> </ul>"},{"location":"running_models/making_a_single_prediction/#using-python","title":"Using Python","text":"<pre><code>&gt;&gt;&gt; from tungstenkit import models\n&gt;&gt;&gt; model = models.get(\"text-to-image\")\n&gt;&gt;&gt; model.predict(\n{\"prompt\": \"a professional photograph of an astronaut riding a horse\"}\n)\n{\"image\": PosixPath(\"./output.png\")}\n</code></pre> <p>You can also make a prediction with multiple inputs. <pre><code>&gt;&gt;&gt; model.predict(\n[{\"prompt\": \"astronaut\"}, {\"prompt\": \"horse\"}]\n)\n[{\"image\": PosixPath(\"./output-0.png\")}, {\"image\": PosixPath(\"./output-1.png\")}]\n</code></pre></p>"},{"location":"running_models/serving_rest_api/","title":"Serving REST API","text":"<p>After building or pulling a model, you can serve a REST API with it.</p>"},{"location":"running_models/serving_rest_api/#starting-a-server","title":"Starting a server","text":""},{"location":"running_models/serving_rest_api/#using-tungstenkit-cli","title":"Using Tungstenkit CLI","text":"<pre><code>$ tungsten serve text-to-image:v1 -p 3000\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>If you want to run with the latest model in a repository, you can omit the tag: <pre><code>$ tungsten serve text-to-image\n</code></pre></p> <p>If you want to run with the latest model, you can omit the model name. <pre><code>$ tungsten serve\n</code></pre></p>"},{"location":"running_models/serving_rest_api/#options","title":"Options","text":"<ul> <li><code>--port (-p)</code>: Bind socket to this port.<ul> <li>Default: <code>3000</code></li> </ul> </li> <li><code>--batch-size</code>: Max batch size for adaptive batching.<ul> <li>Default: Declared value using the <code>define_model</code> decorator</li> </ul> </li> <li><code>--log-level</code>: Log level of the server.<ul> <li>Default: <code>info</code></li> <li>Available values: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code></li> </ul> </li> </ul>"},{"location":"running_models/serving_rest_api/#using-docker","title":"Using Docker","text":"<pre><code>$ docker run --gpus 0 --rm -p 3000:3000 text-to-image:v1\n\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>Refer to Docker docs for more details on the <code>docker run</code> command.</p>"},{"location":"running_models/serving_rest_api/#running-predictions","title":"Running predictions","text":""},{"location":"running_models/serving_rest_api/#starting-a-prediction","title":"Starting a prediction","text":"<p>You can make a prediction with multiple inputs by sending a request to <code>POST /predictions</code> endpoint.</p> <p>The request body should be a list of input JSON objects. Keys of input JSON objects should be same as field names of the input class declared using <code>define_model</code> decorator.</p> <p>The response body has the <code>prediction_id</code> field.</p> <p>For example, let's assume that we defined the input as follows:</p> <pre><code>from tungstenkit import BaseIO, define_model\nclass Input(BaseIO):\nprompt: str\nseed: int\n@define_model(input=Input, ...)\nclass TextToImageModel:\n...\n</code></pre> <p>Then, you can send a request as follows:</p> <pre><code>$ curl -X 'POST' 'http://localhost:3000/predictions' \\\n-H 'Accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"prompt\": \"a professional photograph of an astronaut riding a horse\"}]'\n{\n    \"prediction_id\": \"39c9eb6b\"\n}\n</code></pre>"},{"location":"running_models/serving_rest_api/#getting-the-status-and-result-of-a-prediction","title":"Getting the status and result of a prediction","text":"<p><code>GET /predictions/{prediction_id}</code> endpoint provides the status and result of a prediction.</p> <p>The response is a JSON object with the following fields:</p> <ul> <li><code>outputs</code>: List of output JSON objects. </li> <li><code>status</code>: Among <code>pending</code>, <code>running</code>, <code>success</code> and <code>failed</code>.</li> <li><code>error_message</code>: The error message if <code>status</code> if <code>failed</code>.</li> </ul> <p>For example, let's assume that we defined the output as follows:</p> <pre><code>from tungstenkit import BaseIO, define_model, Image\nclass Output(BaseIO):\nimage: Image\n@define_model(output=Output, ...)\nclass TextToImageModel:\n...\n</code></pre> <p>Then, you can send a request as follows:</p> <pre><code>$ curl -X 'GET' 'http://localhost:3000/predictions/39c9eb6b' \\\n-H 'Accept: application/json'\n{\n    \"outputs\": [{\"image\": \"data:image/png;base64,...\"}],\n    \"status\": \"success\"\n}\n</code></pre>"},{"location":"running_models/serving_rest_api/#canceling-a-prediction","title":"Canceling a prediction","text":"<p>You can cancel a prediction via <code>POST /predictions/{prediction_id}/cancel</code> endpoint.</p> <pre><code>$ curl -X 'GET' 'http://localhost:3000/predictions/39c9eb6b/cancel'\n</code></pre>"},{"location":"running_models/serving_rest_api/#further-information","title":"Further information","text":"<p>See REST API reference for more details and other endpoints.</p>"},{"location":"running_models/using_gpus/","title":"Using GPUs","text":"<p>For running GPU models, Docker should be able to access GPUs. For that, you need to install one of the following:</p> <ul> <li>Linux: nvidia-container-runtime</li> <li>Windows: Docker Desktop WSL 2 backend</li> </ul> <p>After installation, you can check if Docker can access to GPUs using the following command: <pre><code>$ docker run --gpus all --rm ubuntu nvidia-smi\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n|  0%   43C    P8    25W / 420W |      6MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n</code></pre></p>"}]}