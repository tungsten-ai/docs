{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tungsten","text":"<p> The best home for your ML models </p> <p>Tungsten is an open-sourced, developer-frendly tool for building, sharing, testing, and deploying ML models.</p> <p>The key features are:</p> <p>\ud83d\udce6 Framework-agnostic &amp; hastle-free model containerization </p> <ul> <li>No dependencies on specific ML frameworks  </li> <li>No complex configuration files: require only a few lines of Python codes</li> <li>Easy to setup CUDA</li> </ul> <p>\ud83d\ude80 Easy to share models </p> <ul> <li>Automatically generate a RESTful API using FastAPI</li> <li>Provide a clean, intuitive, and locally available web inferface</li> <li>Automatic faas deployment: enable to run predictions remotely right after pushing a model </li> </ul> <p>\u2699\ufe0f Systematic model management (TBA)</p> <ul> <li>Model, test data, and test spec versioning</li> <li>Automatic testing</li> <li>Automatically keep model scores up-to-date</li> </ul> <p>\ud83d\udcb8 Bring your own GPUs</p> <ul> <li>Tungsten Runners: allow your own machines to be used to run remote predictions</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":""},{"location":"#define-a-model","title":"Define a model","text":"<p>Define a Tungsten model in <code>tungsten_model.py</code>: <pre><code>from typing import List, Tuple\nimport torch\nfrom tungstenkit.io import BaseIO, Image\nfrom tungstenkit.model import TungstenModel, config\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@config(\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\ndescription=\"A torch model\"\n)\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\nself.model = torch.load(\"./weights.pth\")\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre></p>"},{"location":"#build-it","title":"Build it","text":"<p>Containerize the model: <pre><code>tungsten build\n</code></pre></p>"},{"location":"#run-it-locally","title":"Run it locally","text":"<p>Now you can run predictions locally: <pre><code># Start the web demo\ntungsten demo\n\n# Start the prediction service\ntungsten serve\n</code></pre></p>"},{"location":"#push-it","title":"Push it","text":"<p>Also, you can push the model to a Tungsten server: <pre><code># Login to a Tungsten server\ntungsten login https://tungsten.example.com\n\n# Push a model\ntungsten push exampleuser/exampleproject\n</code></pre></p>"},{"location":"#run-it-remotely","title":"Run it remotely","text":"<p>Now you can run the model remotely in the web.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.7</li> <li>Docker</li> <li>(Optional) nvidia-docker for running GPU models locally. You can build and push a GPU model without a GPU and nvidia-docker.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install tungstenkit\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Apache License 2.0.</p>"},{"location":"tutorial/getting_started/","title":"Getting Started","text":""},{"location":"tutorial/getting_started/#installation","title":"Installation","text":"<p>The first step is to install Tungstenkit.</p> <p>The prerequisites for installing tungstenkit are:</p> <ul> <li>Python &gt;= 3.7</li> <li>Docker</li> </ul> <p>If they are ready, you can install Tungstenkit as follows:</p> <pre><code>pip install tungstenkit\n</code></pre>"},{"location":"tutorial/getting_started/#run-an-example-model","title":"Run an example model","text":""},{"location":"tutorial/getting_started/#create-a-directory","title":"Create a directory","text":"<p>Let's start by creating a working directory: <pre><code>mkdir tungsten-quickstart\ncd tungsten-quickstart\n</code></pre></p>"},{"location":"tutorial/getting_started/#build-a-model","title":"Build a model","text":"<p>To build a Tungsten model, you should define your input, output, setup &amp; predict functions, and dependencies in <code>tungsten_model.py</code> file.</p> <p>For example, you can define them for an image classification model like this: <pre><code>from typing import List\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit.io import BaseIO, Image\nfrom tungstenkit.model import TungstenModel, config\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@config(\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\n)\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\nself.model = MobileNetV2()\nself.model.load_state_dict(torch.load(\"mobilenetv2_weights.pth\"))\nself.model.eval()\nself.labels = MobileNet_V2_Weights.IMAGENET1K_V2.meta[\"categories\"]\nself.transforms = transforms.Compose(\n[\ntransforms.Resize((224, 224)),\ntransforms.PILToTensor(),\ntransforms.ConvertImageDtype(torch.float),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n]\n)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = self._preprocess(inputs)\nsoftmax = self.model(input_tensor).softmax(1)\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [self.labels[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\ndef _preprocess(self, inputs: List[Input]) -&gt; torch.Tensor:\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [self.transforms(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nreturn input_tensor\n</code></pre> Copy that to a file <code>tungsten_model.py</code>.</p> <p>To setup a model, you should download model weights: <pre><code>curl -o mobilenetv2_weights.pth https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\n</code></pre></p> <p>Now, you can build a model using the definition: <pre><code>tungsten build\n</code></pre></p> <p>You can see that the model you've just created is added to the model list: <pre><code>tungsten models\n</code></pre></p>"},{"location":"tutorial/getting_started/#run-locally","title":"Run locally","text":"<p>Now, you can test the model in your local machine by running predictions.</p> <p>Tungstenkit provides multiple options for that.</p>"},{"location":"tutorial/getting_started/#option-1-an-interactive-web-demo","title":"Option 1: an interactive web demo","text":"<p><pre><code>tungsten demo -p 8080\n</code></pre> Visit http://localhost:8080 to check.</p>"},{"location":"tutorial/getting_started/#option-2-a-restful-api","title":"Option 2: a RESTful API","text":"<p><pre><code>tungsten serve -p 3000\n</code></pre> Visit http://localhost:3000/docs to check.</p>"},{"location":"tutorial/getting_started/#run-remotely","title":"Run remotely","text":"<p>To do this, you should have an account and an entered project in a Tungsten server running at https://tungsten-ai.com.  </p> <p>If you have them, let's login first. <pre><code>tungsten login https://tungsten-ai.com\n</code></pre></p> <p>Then, you can push the built model: <pre><code>tungsten push &lt;username&gt;/&lt;project name&gt;\n</code></pre></p> <p>Now you can visit https://tungsten-ai.com and find a new model is added to the project.</p> <p>You can run it in the web.</p> <p>Also, you can pull the model as follows: <pre><code>tungsten pull &lt;username&gt;/&lt;project name&gt;:&lt;model version&gt;\n</code></pre></p>"},{"location":"tutorial/getting_started/#upgrade-the-example","title":"Upgrade the example","text":""},{"location":"tutorial/inputs_and_outputs/","title":"Inputs and Outputs","text":""},{"location":"tutorial/inputs_and_outputs/#the-baseio-class","title":"The BaseIO class","text":""},{"location":"tutorial/inputs_and_outputs/#files","title":"Files","text":""},{"location":"tutorial/inputs_and_outputs/#supported-types","title":"Supported types","text":""},{"location":"tutorial/tungsten_model/","title":"The Tungsten Model","text":""},{"location":"tutorial/tungsten_model/#declare-input-output-types","title":"Declare input &amp; output types","text":""},{"location":"tutorial/tungsten_model/#define-how-a-prediction-works","title":"Define how a prediction works","text":""},{"location":"tutorial/tungsten_model/#define-how-to-setup-a-model","title":"Define how to setup a model","text":""},{"location":"tutorial/tungsten_model/#configure-a-model","title":"Configure a model","text":""}]}