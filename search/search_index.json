{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#what-is-tungsten","title":"What is Tungsten?","text":"<p>Tungsten is an open-source ML containerization tool/platform with a focus on developer productivity and collaboration.</p> <p>Tungsten builds a versatile and standardized container for your model. Once built, it can run as a REST API server, GUI application, CLI application, serverless function, or scriptable Python function.</p> <p>We also provide a server application for managing and sharing your ML models. It currently supports remote execution, test automation as well as basic versioning feature.</p> <p> </p>"},{"location":"#tungsten-model","title":"Tungsten Model","text":"<p>The Tungsten model packages up everything required to run your model, and exposes a standardized API to support convenient features.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy: Requires only a few lines of Python code.</li> <li>Versatile: Supports multiple usages:<ul> <li>RESTful API server</li> <li>GUI application</li> <li>Serverless function</li> <li>CLI application (coming soon)</li> <li>Python function (coming soon)</li> </ul> </li> <li>Abstracted: User-defined JSON input/output.</li> <li>Standardized: Supports advanced workflows.</li> <li>Scalable: Supports adaptive batching and clustering (coming soon).</li> </ul> <p>See Tungsten Model - Getting Started to learn more.</p>"},{"location":"#take-the-tour","title":"Take the tour","text":""},{"location":"#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Building a Tungsten model is easy. All you have to do is write a simple <code>tungsten_model.py</code> like below:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\nprompt: str\nclass Output(io.BaseIO):\nimage: io.Image\n@model.config(\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=4,\ndescription=\"Text to image\"\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"./weights.pth\")\nself.model = load_torch_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> <p>Now, you can start a build process with the following command: <pre><code>$ tungsten build\n\n\u2705 Successfully built tungsten model: 'text-to-image:latest'\n</code></pre></p>"},{"location":"#run-it-as-a-restful-api-server","title":"Run it as a RESTful API server","text":"<p>You can start a prediction with a REST API call.</p> <p>Start a server:</p> <pre><code>$ docker run -p 3000:3000 --gpus all text-to-image:latest\n\nINFO:     Setting up the model\nINFO:     Getting inputs from the input queue\nINFO:     Starting the prediction service\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>Send a prediction request with a JSON payload:</p> <pre><code>$ curl -X 'POST' 'http://localhost:3000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"prompt\": \"a professional photograph of an astronaut riding a horse\"}]'\n{\n    \"outputs\": [{\"image\": \"data:image/png;base64,...\"}],\n}\n</code></pre>"},{"location":"#run-it-as-a-gui-application","title":"Run it as a GUI application","text":"<p>If you need a more user-friendly way to make predictions, start a GUI app with the following command:</p> <pre><code>$ tungsten demo text-to-image:latest -p 8080\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n</code></pre> <p></p>"},{"location":"#run-it-as-a-serverless-function","title":"Run it as a serverless function","text":"<p>We support remote, serverless executions via a Tungsten server.</p> <p>Push a model:</p> <pre><code>$ tungsten push exampleuser/exampleproject -n text-to-image:latest\n\n\u2705 Successfully pushed to 'https://server.tungsten-ai.com'\n</code></pre> <p>Now, you can start a remote prediction in the Tungsten server:</p> <p></p>"},{"location":"#tungsten-server","title":"Tungsten Server","text":"<p>The Tungsten server provides a platform where you can store, run, and test models.</p>"},{"location":"#key-features_1","title":"Key Features","text":"<ul> <li>Function-as-a-Service (FaaS)</li> <li>Scale with your own GPU/CPU devices</li> <li>Project management</li> <li>Automated testing for CI/CD (coming soon)</li> </ul> <p>See Tungsten Server - Getting Started to learn more.</p>"},{"location":"#take-the-tour_1","title":"Take the tour","text":""},{"location":"#function-as-a-service-faas","title":"Function-as-a-Service (FaaS)","text":"<p>The Tungsten server supports executing models as serverless functions.</p> <p>In a browser, you can test any uploaded model:</p> <p></p> <p>Also, it is possible to make a prediction through the Tungsten server's REST API:</p> <pre><code>$ curl -X 'POST' \\\n'https://server.tungsten-ai.com/api/v1/projects/tungsten/text-to-image/models/2910c07e/predict' \\\n-H 'Authorization: ************' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\"input\": {\"prompt\": \"a professional photograph of an astronaut riding a horse\"}}'\n{\n  \"id\": \"c88e7de9\",\n  \"status\": \"running\",\n}\n$ curl -X 'GET' \\\n'https://server.tungsten-ai.com/api/v1/predictions/c88e7de9' \\\n-H 'Authorization: ************' \\\n-H 'accept: application/json' \\\n{\n  \"output\": {\n    \"image\": \"https://server.tungsten-ai.com/api/v1/files/1/93fd2ac4/output.png\"\n  },\n  \"status\": \"success\"\n}\n</code></pre>"},{"location":"#scale-with-your-own-gpucpu-devices","title":"Scale with your own GPU/CPU devices","text":"<p>You can easily scale serverless infrastructure with your Tungsten runners.</p> <p>Register one or more runners with the following command:</p> <pre><code>$ tungsten-runner register\n\nEnter runner mode (pipeline, prediction) [prediction]: prediction\nEnter URL of the tungsten server: https://server.tungsten-ai.com\nEnter registration token: C6r5rp2PhfdXbJtFbBMhifgLDhagAc\nEnter runner name [mydesktop]: myrunner \nEnter tags (comma separated) []: NVIDIA-A100\nEnter GPU index to use []: 0\nRunner 'myrunner' is registered - id: 245\nUpdated runner config\n</code></pre> <p>Then, start the runners to fetch jobs:</p> <pre><code>$ tungsten-runner run\n\nRunner 0   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n                      2023-04-21 16:59:49.184 | INFO     | Job 0f7c50867417456ebd1389cfb74e489f assigned\nRunner 1   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n</code></pre>"},{"location":"#project-management","title":"Project management","text":"<p>In a Tungsten server, you can organize models by grouping them into projects. </p> <p>Multiple settings are unified in a project:</p> <ul> <li>Input/output schemas</li> <li>Evaluation metrics</li> <li>Test cases</li> <li>Test data</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Apache License 2.0.</p>"},{"location":"tungsten_model/getting_started/","title":"Getting Started","text":""},{"location":"tungsten_model/getting_started/#installation","title":"Installation","text":"<p>The first step is to install Tungstenkit.</p> <p>The prerequisites are:</p> <ul> <li>Python 3.7+</li> <li>Docker</li> </ul> <p>If they are ready, you can install Tungstenkit as follows:</p> <pre><code>pip install tungstenkit\n</code></pre>"},{"location":"tungsten_model/getting_started/#run-an-example-model","title":"Run an example model","text":""},{"location":"tungsten_model/getting_started/#create-a-directory","title":"Create a directory","text":"<p>Let's start by creating a working directory: <pre><code>mkdir tungsten-getting-started\ncd tungsten-getting-started\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#write-tungsten_modelpy","title":"Write <code>tungsten_model.py</code>","text":"<p>You can write the <code>tungsten_model.py</code> file for an image classification model as follows: <pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import io, model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str = io.Field(choices=LABELS)\n@model.config(\ngpu=False,\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\n\"\"\"Load a model into the memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#download-weights","title":"Download weights","text":"<p>Before building, you should prepare the required files.</p> <p>As you can see above, two files are needed: <code>imagenet_labels.json</code> and <code>mobilenetv2_weights.pth</code>. Download these files via the script below: <pre><code>curl -o imagenet_labels.json -X GET https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json &amp;&amp; \\  curl -o mobilenetv2_weights.pth https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Now everything is ready. Let's start building a Tungsten model: <pre><code>$ tungsten build -n tungsten-example\n\n\u2705 Successfully built tungsten model: 'tungsten-example:e3a5de5616a743fe9021e2dcfe1cd19a' (also tagged as 'tungsten-example:latest')\n</code></pre></p> <pre><code>$ tungsten models\n\nRepository        Tag                               Description                 Model Class           Created              Docker Image ID\n----------------  --------------------------------  --------------------------  --------------------  -------------------  -----------------\ntungsten-example  latest                            Image classification model  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\ntungsten-example  e3a5de5616a743fe9021e2dcfe1cd19a  Image classification model  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\n</code></pre>"},{"location":"tungsten_model/getting_started/#run-locally","title":"Run locally","text":"<p>Now, you can run the model in your local machine in multiple ways.</p>"},{"location":"tungsten_model/getting_started/#option-1-an-interactive-web-demo","title":"Option 1: an interactive web demo","text":"<p><pre><code>tungsten demo tungsten-example -p 8080\n</code></pre> Visit http://localhost:8080 to check:</p> <p></p>"},{"location":"tungsten_model/getting_started/#option-2-a-restful-api","title":"Option 2: a RESTful API","text":"<p>Start the server: <pre><code>$ tungsten serve tungsten-example -p 3000\nINFO:     Setting up the model\nINFO:     Getting inputs from the input queue\nINFO:     Starting the prediction service\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre></p> <p>Send a prediction request with a JSON payload: <pre><code>$ curl -X 'POST' 'http://localhost:3000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"image\": \"https://picsum.photos/200.jpg\"}]'\n{\n    \"outputs\": [{\"scale\": 0.12483298, \"label\": \"web site\"}],\n}\n</code></pre></p> <p>Also, you can find a Swagger documentation at http://localhost:3000/docs.</p>"},{"location":"tungsten_model/getting_started/#run-remotely","title":"Run remotely","text":"<p>To do this, you should have an account and a project on a Tungsten server running at https://server.tungsten-ai.com.  </p> <p>If you don't have them, visit https://webapp.tungsten-ai.com in a browser and create them.</p> <p>First, log in: <pre><code>$ tungsten login\n\nUser (username or email): exampleuser\nPassword: \n</code></pre></p> <p>Then, push the model: <pre><code>$ tungsten push exampleuser/exampleproject -n tungsten-example\n\n\u2705 Successfully pushed 'tungsten-example:latest' to 'https://server.tungsten-ai.com'\n  - project: exampleuser/exampleproject\n  - version: 98acfab3\n</code></pre></p> <p>Now you can find and run it on the Tungsten server.</p> <p>Visit https://webapp.tungsten-ai.com in a browser to check it.</p>"},{"location":"tungsten_model/input_and_output/","title":"Input/Output","text":"<p>You can define input and output for image classification like this:</p> <pre><code>from tungstenkit import io\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str\n</code></pre> <p>Here you see:</p> <ul> <li>Input and output classes are inherited from <code>tungstenkit.io.BaseIO</code>.</li> <li>Input has a field named 'image', and its type is <code>tungstenkit.io.Image</code>.</li> <li>Output has two fields named 'score' and 'label', and their types are <code>float</code> and <code>str</code>.</li> </ul>"},{"location":"tungsten_model/input_and_output/#the-baseio-class","title":"The <code>BaseIO</code> class","text":"<p>The <code>BaseIO</code> class is the base class for all inputs and outputs in Tungstenkit. </p> <p>It is a simple wrapper of the <code>BaseModel</code> class in Pydantic, so you can use all useful methods and attributes it provides.</p>"},{"location":"tungsten_model/input_and_output/#supported-field-types","title":"Supported field types","text":"<p>Tungstenkit currently supports the following input/output field types:</p> Type Input Output <code>tungstenkit.io.Image</code> \u2705 \u2705 <code>tungstenkit.io.Video</code> \u2705 \u2705 <code>tungstenkit.io.Audio</code> \u2705 \u2705 <code>tungstenkit.io.Binary</code> \u2705 \u2705 <code>str</code> \u2705 \u2705 <code>float</code> \u2705 \u2705 <code>int</code> \u2705 \u2705 <code>bool</code> \u2705 \u2705 <code>dict</code> or <code>typing.Dict</code> \u274c \u2705 <code>list</code> or <code>typing.List</code> \u274c \u2705 <code>tuple</code> or <code>typing.Tuple</code> \u274c \u2705 A subclass of <code>tungstenkit.io.BaseIO</code> \u274c \u2705 <p>For <code>dict</code>, <code>list</code>, <code>tuple</code>, <code>typing.Dict</code>, <code>typing.List</code> and <code>typing.Tuple</code>, type arguments are required. For example, you should use <code>dict[str, str]</code> instead of <code>dict</code>.</p>"},{"location":"tungsten_model/input_and_output/#files","title":"Files","text":"<p>The <code>tungstenkit.io</code> module provides four primitives for files: <code>Image</code>, <code>Video</code>, <code>Audio</code>, and <code>Binary</code>. They possess the following property and method:</p> <ul> <li><code>path</code> : a string of the file path.</li> <li><code>from_path(path: StrPath)</code>: a class method for creating file objects from a filepath. <pre><code>&gt;&gt;&gt; from tungstenkit import io\n&gt;&gt;&gt; video_path = \"video.mp4\"\n&gt;&gt;&gt; video = io.Video.from_path(video_path)\n&gt;&gt;&gt; video.path\n'/home/tungsten/working_dir/video.mp4'\n</code></pre></li> </ul> <p>The <code>Image</code> object has more methods:</p> <ul> <li><code>from_pil_image(pil_image: PIL.Image.Image)</code>: a class method for creating the object from a <code>PIL.Image.Image</code> object.</li> <li><code>to_pil_image(mode: str = \"RGB\")</code>: returns a <code>PIL.Image.Image</code> object.</li> </ul>"},{"location":"tungsten_model/input_and_output/#input-field-descriptors","title":"Input field descriptors","text":"<p>The <code>tungstenkit.io</code> module contains two input field descriptors: <code>Field</code> and <code>Option</code> functions:</p> <ul> <li><code>Field</code>: For setting properties of a required field.</li> <li><code>Option</code>: For declaring a field as optional and setting its properties. Optional fields will be same in an input batch and hidden in the model demo page by default.</li> </ul> <p>Using them, you can:</p> <ul> <li>Distinguish between required and optional fields.</li> <li>Restrict input field values.</li> <li>Set input field descriptions shown in the model demo page.</li> </ul> <p>For example, you can define an input class for text-to-image generation as follows: <pre><code>from tungstenkit import io\nclass Input(io.BaseIO):\nprompt: str = io.Field(\ndescription=\"Input prompt\", \nmin_length=1, \nmax_length=200\n)\nwidth: int = io.Option(\ndescription=\"Width of output image\",\nchoices=[128, 256, 512],\ndefault=512,\n)\nheight: int = io.Option(\ndescription=\"Height of output image\",\nchoices=[128, 256, 512],\ndefault=512,\n)\n</code></pre></p> <p>Both field descriptors take the following keyword-only arguments:</p> <ul> <li><code>description</code> (<code>str</code>, optional): Human-readable description.</li> <li><code>ge</code> (<code>float</code>, optional): Greater than or equal. If set, value must be greater than or equal to this. Only applicable to <code>int</code> and <code>float</code>.</li> <li><code>le</code> (<code>float</code>, optional): Less than or equal. If set, value must be less than or equal to this. Only applicable to <code>int</code> and <code>float</code>.</li> <li><code>min_length</code> (<code>int</code>, optional): Minimum length for strings.</li> <li><code>max_length</code> (<code>int</code>, optional): Maximum length for strings.</li> <li><code>choices</code> (<code>list</code>, optional): List of possible values. If set, value must be among a value in this.</li> </ul> <p>The <code>Option</code> function additionally takes a positional argument:</p> <ul> <li><code>default</code> (<code>Any</code>, required): Default value.</li> </ul>"},{"location":"tungsten_model/install_tungstenkit/","title":"Install tungstenkit","text":""},{"location":"tungsten_model/install_tungstenkit/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>Docker</li> <li>(Optional) Install nvidia-docker if you want to run GPU models locally.</li> </ul>"},{"location":"tungsten_model/install_tungstenkit/#installation","title":"Installation","text":"<pre><code>pip install tungstenkit\n</code></pre>"},{"location":"tungsten_model/the_tungsten_model_class/","title":"The TungstenModel Class","text":"<p>The <code>TungstenModel</code> class is the base class for all Tungsten model classes. A Tungsten model class looks something like this:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\n...\nclass Output(io.BaseIO):\n...\n@model.config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(model.TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre>"},{"location":"tungsten_model/the_tungsten_model_class/#basic-usage","title":"Basic Usage","text":""},{"location":"tungsten_model/the_tungsten_model_class/#declare-inputoutput-types","title":"Declare input/output types","text":"<p>Input/output types are declared by passing them as type arguments to <code>TungstenModel</code> class:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\n...\nclass Output(io.BaseIO):\n...\n@model.config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(model.TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>See Tungsten Model - Input/Output to learn how to define input/output.</p>"},{"location":"tungsten_model/the_tungsten_model_class/#define-how-to-load-a-model","title":"Define how to load a model","text":"<p>You can override the <code>setup</code> method to define how to load a model:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\n...\nclass Output(io.BaseIO):\n...\n@model.config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(model.TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>As you can see, the <code>weights.pth</code> file is required to setup. Before building, you should make sure that the file exists in the build directory.</p>"},{"location":"tungsten_model/the_tungsten_model_class/#define-how-a-prediction-works","title":"Define how a prediction works","text":"<p>The <code>predict</code> method defines the computation performed at every prediction request.</p> <p>It takes a non-empty list of <code>Input</code> objects as an argument, and should return a list of the same number of <code>Output</code> objects.</p> <p>It should be overridden by all subclasses:</p> <pre><code>from typing import List\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\n...\nclass Output(io.BaseIO):\n...\n@model.config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(model.TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre> <p>During runtime, Tungstenkit automatically keeps all options same in a batch. So, you can define the <code>predict</code> method of a text-to-image generaton model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import io\nclass Input(io.BaseIO):\nprompt: str\nwidth: int = io.Option(\nchoices=[128, 256],\ndefault=256,\n)\nheight: int = io.Option(\nchoices=[128, 256],\ndefault=256,\n)\nclass Output(io.BaseIO):\nimage: io.Image\nclass Model(model.TungstenModel[Input, Output]):\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\noptions = inputs[0]\nprompts = [inp.prompt for inp in inputs]\npil_images = self.model(prompts, width=options.width, height=options.height)\nimages = [io.Image.from_pil_image(pil_image) for pil_image in pil_images]\noutputs = [Output(image=image) for image in images]\nreturn outputs\n</code></pre>"},{"location":"tungsten_model/the_tungsten_model_class/#add-dependencies-and-explanations","title":"Add dependencies and explanations","text":"<p>You can add dependencies and explanations via the <code>config</code> decorator: <pre><code>from typing import List\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\n...\nclass Output(io.BaseIO):\n...\n@model.config(\ngpu=True,  # Whether to use a GPU or not\nbatch_size=4,  # Max batch size for adaptive batching\npython_packages=[\"torch\", \"torchvision\"],  # Required Python packages\ndescription=\"An example model\"  # Model description\n)\nclass Model(model.TungstenModel[Input, Output]):\n\"\"\"\n    A Tungsten model whose input/output types are 'Input' and 'Output', respectively.\n    \"\"\"\ndef setup(self):\n\"\"\"Load model weights\"\"\"\nweights = torch.load(\"weights.pth\")\nself.model = load_model(weights)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\ninputs = preprocess(inputs)\noutputs = self.model(inputs)\noutputs = postprocess(outputs)\nreturn outputs\n</code></pre></p> <p>The <code>config</code> decorator takes the following keyword-only arguments:</p> <ul> <li><code>description (str | None)</code>: A text explaining the model (default: <code>None</code>).</li> <li><code>readme_md (str | None)</code>: Path to the <code>README.md</code> file (default: <code>None</code>).</li> <li><code>batch_size (int)</code>: Max batch size for adaptive batching (default: <code>1</code>).</li> <li><code>gpu (bool)</code>: Indicates if the model requires GPUs (default: <code>False</code>).</li> <li><code>cuda_version (str | None)</code>: CUDA version in <code>&lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;]]</code> format. If <code>None</code> (default), the cuda version will be automatically determined as compatible with <code>python_packages</code>. Otherwise, fix the CUDA version as <code>cuda_version</code>.</li> <li><code>gpu_mem_gb (int)</code>: Minimum GPU memory size required to run the model (default: <code>16</code>). This argument will be ignored if <code>gpu==False</code>.</li> <li><code>python_packages (list[str] | None)</code>: A list of pip requirements in <code>&lt;name&gt;[==&lt;version&gt;]</code> format. If <code>None</code> (default), no extra Python packages are added.</li> <li><code>python_version (str | None)</code>: Python version to use in <code>&lt;major&gt;[.&lt;minor&gt;]</code> format. If <code>None</code> (default), the python version will be automatically determined as compatible with <code>python_packages</code> while prefering the current Python version. Otherwise, fix the Python version as <code>python_version</code>.</li> <li><code>system_packages (list[str] | None)</code>: A list of system packages that will installed by the system package manager (e.g. <code>apt</code>). The default value is <code>None</code>. This argument will be ignored while using a custom base image, because Tungstenkit cannot decide which package manager to use.</li> <li><code>mem_gb (int)</code>: Minimum memory size required to run the model (default: <code>8</code>).</li> <li><code>include_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code>. If <code>None</code> (default), all files in the working directory and its subdirectories are added, which is equivalent to <code>[*]</code>.</li> <li><code>exclude_files (list[str] | None)</code>: A list of patterns as in <code>.gitignore</code> for matching which files to exclude. If <code>None</code> (default), all hidden files and Python bytecodes are ignored, which is equivalent to <code>[\".*/\", \"__pycache__/\", \"*.pyc\", \"*.pyo\", \"*.pyd\"]</code>.</li> <li><code>dockerfile_commands (list[str] | None)</code>: A list of dockerfile commands (default: <code>None</code>). The commands will be executed before setting up python packages.</li> <li><code>base_image (str | None)</code>: Base docker image in <code>&lt;repository&gt;[:&lt;tag&gt;]</code> format. If <code>None</code> (default), the base image is automatically selected with respect to pip packages, the device type, and the CUDA version. Otherwise, use it as the base image and <code>system_packages</code> will be ignored.</li> </ul>"},{"location":"tungsten_model/the_tungsten_model_class/#advanced-usage","title":"Advanced Usage","text":""},{"location":"tungsten_model/the_tungsten_model_class/#define-how-a-demo-prediction-works","title":"Define how a demo prediction works","text":"<p>You can define an object detection model like this:</p> <pre><code>from typing import List\nfrom tungstenkit import io\nclass BoundingBox(io.BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(io.BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\ndetections: List[Detection]\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>But it doesn't contain any visualization, so you'll only get raw JSONs on the demo page.</p> <p>Then, you could add the visualization result to the output:</p> <pre><code>from typing import List\nfrom tungstenkit import io\nclass BoundingBox(io.BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(io.BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\ndetections: List[Detection]\nvisualized: io.Image\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\n</code></pre> <p>This can improve the demo page, but introduces the visualization overhead of the API.</p> <p>In such a case, you can separate the method for demo predictions:</p> <p><pre><code>from typing import List, Tuple, Dict\nfrom tungstenkit import io\nclass BoundingBox(io.BaseIO):\nxmin: int\nxmax: int\nymin: int\nymax: int\nclass Detection(io.BaseIO):\nlabel: str\nbbox: BoundingBox\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\ndetections: List[Detection]\nclass Visualization(io.BaseIO):\nresult: io.Image\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\n...\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n...\ndef predict_demo(\nself, \ninputs: List[Input]\n) -&gt; Tuple[List[Output], List[Visualization]]:\noutputs = self.predict(inputs)\npil_images = visualize(inputs, outputs)\nimages = [io.Image.from_pil_image(pil_image) for pil_image in pil_images]\nvisualizations = [Visualization(result=image) for image in images]\nreturn outputs, visualizations\n</code></pre> Then, the <code>predict</code> method is executed when a prediction is requested through the API, but the <code>predict_demo</code> method is called for a demo request.</p>"},{"location":"tungsten_model/use_gpus/","title":"Use GPUs","text":""},{"location":"tungsten_model/use_gpus/#prerequisite","title":"Prerequisite","text":"<p>You should install nvidia-docker if you want to run GPU models locally.</p> <p>But you can still build, push, and pull GPU models without it.</p>"},{"location":"tungsten_model/use_gpus/#declare-as-a-gpu-model","title":"Declare as a GPU model","text":"<p>You can set <code>gpu=True</code> in the <code>tungstenkit.model.config</code> decorator:</p> <p><pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import io, model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str = io.Field(choices=LABELS)\n@model.config(\ngpu=True,\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\n\"\"\"Load a model into the memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre> Then, Tungstenkit automatically selects a compatible CUDA version and installs it in the container. The CUDA version inference is currently supported on <code>torch</code>, <code>torchvision</code>, <code>torchaudio</code>, and <code>tensorflow</code>.</p>"},{"location":"tungsten_model/use_gpus/#manually-set-the-cuda-version","title":"Manually set the CUDA version","text":"<p>You can also pass <code>cuda_version</code> as an argument of the <code>tungstenkit.model.config</code> decorator:</p> <pre><code>import json\nfrom pathlib import Path\nfrom typing import List\nimport torch\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import io, model\nLABELS = json.loads(Path(\"imagenet_labels.json\").read_text())\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str = io.Field(choices=LABELS)\n@model.config(\ngpu=True,\ncuda_version=\"11.6\",\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=16,\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\n\"\"\"Load a model into the memory\"\"\"\nself.model = MobileNetV2()\nweights = torch.load(\"mobilenetv2_weights.pth\")\nself.model.load_state_dict(weights)\nself.model.eval()\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\n\"\"\"Run a batch prediction\"\"\"\nprint(\"Preprocessing\")\ntransform = MobileNet_V2_Weights.IMAGENET1K_V2.transforms()\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [transform(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nprint(\"Inferencing\")\nsoftmax = self.model(input_tensor).softmax(1)\nprint(\"Postprocessing\")\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [LABELS[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\n</code></pre>"},{"location":"tungsten_server/getting_started/","title":"Getting Started","text":"<p>Coming soon</p>"}]}