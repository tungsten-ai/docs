{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tungsten","text":""},{"location":"#what-is-tungsten","title":"What is Tungsten?","text":"<p>Tungsten is a containerization tool and platform for easily sharing and managing ML models.</p> <p>Tungsten enables to build a versatile and standardized container for an ML model. Without any model-specific setup, it can be run as a RESTful API server, a GUI application, a CLI application, and a serverless function, and a Python script.</p> <p>Also, Tungsten provides a centralized place to manage ML models systematically. It supports remote execution and test automation as well as storing models.</p>"},{"location":"#tungsten-model","title":"Tungsten Model","text":"<p>The Tungsten model is the basic unit of ML model in Tungsten. It is a Docker container and contains a standardized API and all dependencies for an ML model.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy to build: Require only a few lines of Python codes.</li> <li>Easy to use: Do not require any model-specific setup for running.</li> <li>Versatile: Can be used in multiple ways:<ul> <li>RESTful API server</li> <li>GUI application</li> <li>Serverless function</li> <li>CLI application (coming soon)</li> <li>Within Python scripts (coming soon)</li> </ul> </li> <li>Standardized: Communicate with JSONs through a standardized RESTful API.</li> <li>Scalable: Support adaptive batching and clustering with Redis and a cloud storage (coming soon).</li> </ul> <p>For learning more with a complete example, see the Tungsten Model - Getting Started.</p>"},{"location":"#take-the-tour","title":"Take the tour","text":""},{"location":"#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Building a Tungsten model does not require any complex configuration file for building. </p> <p>All you have to do is write a simple <code>tungsten_model.py</code> like below: <pre><code>from typing import List, Tuple\nimport torch\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str\n@model.config(\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\ndescription=\"Image classification model\"\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\nself.model = torch.load(\"./weights.pth\")\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre></p>"},{"location":"#run-it-as-a-restful-api-server","title":"Run it as a RESTful API server","text":"<p>A Tungsten model includes a standardized RESTful API.</p> <p>Run the container:</p> <pre><code>$ docker run -p 3000:3000 --gpus all tungsten-example:latest\n\nINFO:     Setting up the model\nINFO:     Getting inputs from the input queue\nINFO:     Starting the prediction service\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>Then, you can send a prediction request with a JSON payload. For example, <pre><code>$ curl -X 'POST' 'http://localhost:3000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"image\": \"https://picsum.photos/200.jpg\"}]'\n{\n    \"status\": \"success\",\n    \"outputs\": [{\"score\": 0.5, \"label\": \"dog\"}],\n    \"error_message\": null\n}\n</code></pre></p> <p>Also, a Swagger documentation is automatically generated. You can find it in <code>/docs</code> endpoint:</p> <p></p>"},{"location":"#run-it-as-a-gui-application","title":"Run it as a GUI application","text":"<p>With a Tungsten model, you can run a GUI app in a single command: <pre><code>$ tungsten demo tungsten-example:latest -p 8080\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n</code></pre></p> <p></p>"},{"location":"#run-it-as-a-serverless-function","title":"Run it as a serverless function","text":"<p>Push a model to a Tungsten platform: <pre><code>$ tungsten push exampleuser/exampleproject -n tungsten-example:latest\n\n\u2705 Successfully pushed to 'https://server.tungsten-ai.com'\n</code></pre></p> <p>Then you can run it in the Tungsten platform:</p> <p></p>"},{"location":"#tungsten-platform","title":"Tungsten Platform","text":"<p>The Tungsten platform is where you store, run, test, and compare ML models.</p>"},{"location":"#key-features_1","title":"Key Features","text":"<ul> <li>Hassle-free model deployment</li> <li>Allow your own machines to be used to run models</li> <li>Model, test data, and test spec versioning (comming soon)</li> <li>Automatically keep evaluation scores up-to-date (comming soon)</li> </ul>"},{"location":"#take-the-tour_1","title":"Take the tour","text":""},{"location":"#hassle-free-model-deployment","title":"Hassle-free model deployment","text":"<p>The Tungsten platform supports automatic serverless deployment of models. So, you don't need to spend time managing infrastructure for serving them.</p> <p>You can run all uploaded models through Tungsten platform's API or web UI.</p>"},{"location":"#allow-your-own-machines-to-be-used-to-run-models","title":"Allow your own machines to be used to run models","text":"<p>You can register Tungsten runners to a Tungsten server and make the server use your own machines for running models.</p> <p>Register a runner:</p> <pre><code>$ tungsten-runner register\n\nEnter runner mode (pipeline, prediction) [prediction]: prediction\nEnter URL of the tungsten server: https://server.tungsten-ai.com\nEnter registration token: C6r5rp2PhfdXbJtFbBMhifgLDhagAc\nEnter runner name [mydesktop]: myrunner \nEnter tags (comma separated) []: myrunnergroup\nEnter GPU index to use []: 0\nRunner 'mjpyeon-desktop' is registered - id: 245\nUpdated runner config\n</code></pre> <p>Run all registered runners:</p> <pre><code>$ tungsten-runner run\n\nRunner 0   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n                      2023-04-21 16:59:49.184 | INFO     | Job 0f7c50867417456ebd1389cfb74e489f assigned\nRunner 1   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Apache License 2.0.</p>"},{"location":"tungsten_model/getting_started/","title":"Getting Started","text":""},{"location":"tungsten_model/getting_started/#installation","title":"Installation","text":"<p>The first step is to install Tungstenkit.</p> <p>The prerequisites are:</p> <ul> <li>Python &gt;= 3.7</li> <li>Docker</li> <li>(Optional) nvidia-docker for running GPU models locally. You can build and push a GPU model without a GPU and nvidia-docker.</li> </ul> <p>If they are ready, you can install Tungstenkit as follows:</p> <pre><code>pip install tungstenkit\n</code></pre>"},{"location":"tungsten_model/getting_started/#run-an-example-model","title":"Run an example model","text":""},{"location":"tungsten_model/getting_started/#create-a-directory","title":"Create a directory","text":"<p>Let's start by creating a working directory: <pre><code>mkdir tungsten-getting-started\ncd tungsten-getting-started\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#write-tungsten_modelpy","title":"Write <code>tungsten_model.py</code>","text":"<p>To build a Tungsten model, you should define your input, output, setup &amp; predict functions, and dependencies in <code>tungsten_model.py</code> file.</p> <p>For example, you can define them for an image classification model like this: <pre><code>from typing import List\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit import io, model\nclass Input(io.BaseIO):\nimage: io.Image\nclass Output(io.BaseIO):\nscore: float\nlabel: str\n@model.config(\ngpu=False,\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\n)\nclass Model(model.TungstenModel[Input, Output]):\ndef setup(self):\nself.model = MobileNetV2()\nself.model.load_state_dict(torch.load(\"mobilenetv2_weights.pth\"))\nself.model.eval()\nself.labels = MobileNet_V2_Weights.IMAGENET1K_V2.meta[\"categories\"]\nself.transforms = transforms.Compose(\n[\ntransforms.Resize((224, 224)),\ntransforms.PILToTensor(),\ntransforms.ConvertImageDtype(torch.float),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n]\n)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = self._preprocess(inputs)\nsoftmax = self.model(input_tensor).softmax(1)\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [self.labels[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\ndef _preprocess(self, inputs: List[Input]) -&gt; torch.Tensor:\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [self.transforms(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nreturn input_tensor\n</code></pre> Copy that to a file <code>tungsten_model.py</code>.</p>"},{"location":"tungsten_model/getting_started/#prepare-the-build-environment","title":"Prepare the build environment","text":"<p>Tungstenkit loads <code>tungsten_model.py</code> to check input/output types and model configuration.  So, install <code>torch</code> and <code>torchvision</code> loaded in <code>tungsten_model.py</code>: <pre><code>pip install torch torchvision\n</code></pre></p> <p>Also, we should prepare files used in <code>setup</code> function of <code>Model</code> class in <code>tungsten_model.py</code>. Refering to the definition, <code>mobilenetv2_weights.pth</code> is required. Let's download it: <pre><code>curl -o mobilenetv2_weights.pth https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Now everything is ready. Let's start building a Tungsten model: <pre><code>$ tungsten build -n tungsten-example\n\n\u2705 Successfully built tungsten model: 'tungsten-example:e3a5de5616a743fe9021e2dcfe1cd19a' (also tagged as 'tungsten-example:latest')\n</code></pre></p> <p>Also you can see that the model is added to the list of models: <pre><code>$ tungsten models\n\nRepository        Tag                               Description       Model Class           Created              Docker Image ID\n----------------  --------------------------------  ----------------  --------------------  -------------------  -----------------\ntungsten-example  latest                            Blur after delay  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\ntungsten-example  e3a5de5616a743fe9021e2dcfe1cd19a  Blur after delay  tungsten_model:Model  2023-04-26 05:23:58  830eb82f0fcd\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#run-locally","title":"Run locally","text":"<p>Now, you can run the model in your local machine.</p> <p>Tungstenkit provides multiple options for that.</p>"},{"location":"tungsten_model/getting_started/#option-1-an-interactive-web-demo","title":"Option 1: an interactive web demo","text":"<p><pre><code>tungsten demo tungsten-example -p 8080\n</code></pre> Visit http://localhost:8080 to check.</p>"},{"location":"tungsten_model/getting_started/#option-2-a-restful-api","title":"Option 2: a RESTful API","text":"<p><pre><code>tungsten serve tungsten-example -p 3000\n</code></pre> Visit http://localhost:3000/docs to check.</p>"},{"location":"tungsten_model/getting_started/#run-remotely","title":"Run remotely","text":"<p>To do this, you should have an account and an entered project in a Tungsten server running at https://tungsten-ai.com.  </p> <p>If you have them, let's login first. <pre><code>tungsten login\n</code></pre></p> <p>Then, you can push the built model: <pre><code>tungsten push &lt;username&gt;/&lt;project name&gt;\n</code></pre></p> <p>Now you can find a new model is added to the project.</p> <p>Visit https://tungsten-ai.com in a browser and run it.</p> <p>Also, you can pull the model as follows: <pre><code>tungsten pull &lt;username&gt;/&lt;project name&gt;:&lt;model version&gt;\n</code></pre></p>"},{"location":"tungsten_model/getting_started/#upgrade-the-example","title":"Upgrade the example","text":""},{"location":"tungsten_model/inputs_and_outputs/","title":"Inputs and outputs","text":""},{"location":"tungsten_model/inputs_and_outputs/#the-baseio-class","title":"The BaseIO class","text":""},{"location":"tungsten_model/inputs_and_outputs/#files","title":"Files","text":""},{"location":"tungsten_model/inputs_and_outputs/#supported-types","title":"Supported types","text":""},{"location":"tungsten_model/tungsten_model/","title":"Tungsten model","text":""},{"location":"tungsten_model/tungsten_model/#declare-input-output-types","title":"Declare input &amp; output types","text":""},{"location":"tungsten_model/tungsten_model/#define-how-a-prediction-works","title":"Define how a prediction works","text":""},{"location":"tungsten_model/tungsten_model/#define-how-to-setup-a-model","title":"Define how to setup a model","text":""},{"location":"tungsten_model/tungsten_model/#configure-a-model","title":"Configure a model","text":""}]}