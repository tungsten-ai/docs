{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tungsten","text":"<p>Tungsten is the easiest way to share and manage ML models.</p> <p>\ud83d\ude80 Build once, use everywhere Tungsten-built ML model containers can be used as RESTful API servers, GUI/CLI applications, serverless functions, and functions in Python scripts without any model-specific setup.</p> <p>\u2699\ufe0f Manage all in one place Tungsten stores every version of ML models, data, and test specs. Also, it automatically runs tests and keeps evaluation scores up-to-date. So, users can easily run, compare, and download ML models.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Build only with a few lines of Python codes</li> <li>Automatically generate a standardized RESTful API for a model</li> <li>Provide a clean and intuitive web UI for a model</li> <li>Allow your own machines to be used to run models</li> <li>Model, test data, and test spec versioning (comming soon)</li> <li>Keep test scores up-to-date (comming soon)</li> </ul> <p>For a complete example including more features, see the Tutorial - Getting Started.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.7</li> <li>Docker</li> <li>(Optional) nvidia-docker for running GPU models locally. But you can build, push, run remotely a GPU model without it.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install tungstenkit\n</code></pre>"},{"location":"#take-the-tour","title":"Take the tour","text":""},{"location":"#build-only-with-a-few-lines-of-python-codes","title":"Build only with a few lines of Python codes","text":"<p>Tungsten does not require any complex configuration file for building. </p> <p>All you have to do is write a simple <code>tungsten_model.py</code> like below: <pre><code>from typing import List, Tuple\nimport torch\nfrom tungstenkit.io import BaseIO, Image\nfrom tungstenkit.model import TungstenModel, config\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@config(\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\ndescription=\"A torch model\"\n)\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\nself.model = torch.load(\"./weights.pth\")\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> Now you can start building: <pre><code>$ tungsten build -n tungsten-example\n\n\u2705 Successfully built tungsten model: 'tungsten-basic:latest'\n</code></pre></p>"},{"location":"#automatically-generate-a-standardized-restful-api-for-a-model","title":"Automatically generate a standardized RESTful API for a model","text":"<p>The model container is a standardized RESTful API server itself.</p> <p>Run the container:</p> <pre><code>$ docker run -p 3000:3000 tungsten-example:latest\n\nINFO:     Setting up the model\nINFO:     Getting inputs from the input queue\nINFO:     Starting the prediction service\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>Now you can run predictions using the server. For example, <pre><code>$ curl -X 'POST' 'http://localhost:3000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"image\": \"https://picsum.photos/200.jpg\"}]'\n{\n    \"status\": \"success\",\n    \"outputs\": [{\"score\": 0.5, \"label\": \"dog\"}],\n    \"error_message\": null\n}\n</code></pre></p> <p>Also, a Swagger documentation for the server is automatically generated.</p> <p>Visit http://localhost:3000/docs in a browser:</p> <p></p>"},{"location":"#provide-a-clean-and-intuitive-web-ui-for-a-model","title":"Provide a clean and intuitive web UI for a model","text":""},{"location":"#option-1-run-remotely","title":"Option 1. Run remotely","text":"<p>First, login to a Tungsten server: <pre><code>$ tungsten login\n\nLogin Success!\n</code></pre> Then, push a model: <pre><code>$ tungsten push &lt;user name&gt;/&lt;project name&gt; -n tungsten-example:latest\n\n\u2705 Successfully pushed to 'https://server.tungsten-ai.com'\n</code></pre></p> <p>Now you can run this model in the Tungsten dashboard.</p> <p>Visit https://tungsten-ai.com in a browser:</p> <p></p>"},{"location":"#option-2-run-locally","title":"Option 2. Run locally","text":"<p>You can run a GUI app locally in a single command: <pre><code>$ tungsten demo tungsten-example:latest -p 8080\n\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n</code></pre></p> <p>Visit http://localhost:8080 in a browser:</p> <p></p>"},{"location":"#allow-your-own-machines-to-be-used-to-run-models","title":"Allow your own machines to be used to run models","text":"<p>You can register Tungsten runners to a Tungsten server and make the server use your own machines for running models.</p> <p>Register a runner:</p> <pre><code>$ tungsten-runner register\n\nEnter runner mode (pipeline, prediction) [prediction]: prediction\nEnter URL of the tungsten server: https://server.tungsten-ai.com\nEnter registration token: C6r5rp2PhfdXbJtFbBMhifgLDhagAc\nEnter runner name [mydesktop]: myrunner \nEnter tags (comma separated) []: myrunnergroup\nEnter GPU index to use []: 0\nRunner 'mjpyeon-desktop' is registered - id: 245\nUpdated runner config\n</code></pre> <p>Run all registered runners:</p> <pre><code>$ tungsten-runner run\n\nRunner 0   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n                      2023-04-21 16:59:49.184 | INFO     | Job 0f7c50867417456ebd1389cfb74e489f assigned\nRunner 1   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Apache License 2.0.</p>"},{"location":"tutorial/getting_started/","title":"Getting Started","text":""},{"location":"tutorial/getting_started/#installation","title":"Installation","text":"<p>The first step is to install Tungstenkit.</p> <p>The prerequisites for installing tungstenkit are:</p> <ul> <li>Python &gt;= 3.7</li> <li>Docker</li> </ul> <p>If they are ready, you can install Tungstenkit as follows:</p> <pre><code>pip install tungstenkit\n</code></pre>"},{"location":"tutorial/getting_started/#run-an-example-model","title":"Run an example model","text":""},{"location":"tutorial/getting_started/#create-a-directory","title":"Create a directory","text":"<p>Let's start by creating a working directory: <pre><code>mkdir tungsten-quickstart\ncd tungsten-quickstart\n</code></pre></p>"},{"location":"tutorial/getting_started/#build-a-model","title":"Build a model","text":"<p>To build a Tungsten model, you should define your input, output, setup &amp; predict functions, and dependencies in <code>tungsten_model.py</code> file.</p> <p>For example, you can define them for an image classification model like this: <pre><code>from typing import List\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit.io import BaseIO, Image\nfrom tungstenkit.model import TungstenModel, config\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@config(\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\n)\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\nself.model = MobileNetV2()\nself.model.load_state_dict(torch.load(\"mobilenetv2_weights.pth\"))\nself.model.eval()\nself.labels = MobileNet_V2_Weights.IMAGENET1K_V2.meta[\"categories\"]\nself.transforms = transforms.Compose(\n[\ntransforms.Resize((224, 224)),\ntransforms.PILToTensor(),\ntransforms.ConvertImageDtype(torch.float),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n]\n)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = self._preprocess(inputs)\nsoftmax = self.model(input_tensor).softmax(1)\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [self.labels[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\ndef _preprocess(self, inputs: List[Input]) -&gt; torch.Tensor:\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [self.transforms(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nreturn input_tensor\n</code></pre> Copy that to a file <code>tungsten_model.py</code>.</p> <p>To setup a model, you should download model weights: <pre><code>curl -o mobilenetv2_weights.pth https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\n</code></pre></p> <p>Now, you can build a model using the definition: <pre><code>tungsten build\n</code></pre></p> <p>You can see that the model you've just created is added to the model list: <pre><code>tungsten models\n</code></pre></p>"},{"location":"tutorial/getting_started/#run-locally","title":"Run locally","text":"<p>Now, you can test the model in your local machine by running predictions.</p> <p>Tungstenkit provides multiple options for that.</p>"},{"location":"tutorial/getting_started/#option-1-an-interactive-web-demo","title":"Option 1: an interactive web demo","text":"<p><pre><code>tungsten demo -p 8080\n</code></pre> Visit http://localhost:8080 to check.</p>"},{"location":"tutorial/getting_started/#option-2-a-restful-api","title":"Option 2: a RESTful API","text":"<p><pre><code>tungsten serve -p 3000\n</code></pre> Visit http://localhost:3000/docs to check.</p>"},{"location":"tutorial/getting_started/#run-remotely","title":"Run remotely","text":"<p>To do this, you should have an account and an entered project in a Tungsten server running at https://tungsten-ai.com.  </p> <p>If you have them, let's login first. <pre><code>tungsten login\n</code></pre></p> <p>Then, you can push the built model: <pre><code>tungsten push &lt;username&gt;/&lt;project name&gt;\n</code></pre></p> <p>Now you can find a new model is added to the project.</p> <p>Visit https://tungsten-ai.com in a browser and run it.</p> <p>Also, you can pull the model as follows: <pre><code>tungsten pull &lt;username&gt;/&lt;project name&gt;:&lt;model version&gt;\n</code></pre></p>"},{"location":"tutorial/getting_started/#upgrade-the-example","title":"Upgrade the example","text":""},{"location":"tutorial/inputs_and_outputs/","title":"Inputs and Outputs","text":""},{"location":"tutorial/inputs_and_outputs/#the-baseio-class","title":"The BaseIO class","text":""},{"location":"tutorial/inputs_and_outputs/#files","title":"Files","text":""},{"location":"tutorial/inputs_and_outputs/#supported-types","title":"Supported types","text":""},{"location":"tutorial/tungsten_model/","title":"The Tungsten Model","text":""},{"location":"tutorial/tungsten_model/#declare-input-output-types","title":"Declare input &amp; output types","text":""},{"location":"tutorial/tungsten_model/#define-how-a-prediction-works","title":"Define how a prediction works","text":""},{"location":"tutorial/tungsten_model/#define-how-to-setup-a-model","title":"Define how to setup a model","text":""},{"location":"tutorial/tungsten_model/#configure-a-model","title":"Configure a model","text":""}]}