{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tungsten","text":""},{"location":"#what-is-tungsten","title":"What is Tungsten?","text":"<p>Tungsten is a containerization tool and platform for easily sharing and managing ML models.</p> <p>Tungsten enables to build a versatile and standardized container for an ML model. Without any model-specific setup, it can be run as a RESTful API server, a GUI application, a CLI application, a serverless function, and a function in Python scripts.</p> <p>Also, Tungsten provides a centralized place to manage ML models systematically. It supports remote execution and test automation for them as well as storing models.</p>"},{"location":"#tungsten-model","title":"Tungsten Model","text":"<p>The Tungsten model is the basic unit of ML model in Tungsten. It is a Docker container and contains a standardized API and all dependencies for an ML model.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Versatile: Can be used in multiple ways:<ul> <li>RESTful API server</li> <li>GUI application</li> <li>CLI application</li> <li>Serverless function</li> <li>Function in Python scripts</li> </ul> </li> <li>Easy to use: Do not require any model-specific setup for running.</li> <li>Easy to build: Require only a few lines of Python codes.</li> <li>Standardized: Contain a standardized RESTful API.</li> <li>Scalable: Support adaptive batching and clustring with Redis and a cloud storage.</li> </ul> <p>For learning more with a complete example, see the Tungsten Model - Getting Started.</p>"},{"location":"#take-the-tour","title":"Take the tour","text":""},{"location":"#build-a-tungsten-model","title":"Build a Tungsten model","text":"<p>Building a Tungsten model does not require any complex configuration file for building. </p> <p>All you have to do is write a simple <code>tungsten_model.py</code> like below: <pre><code>from typing import List, Tuple\nimport torch\nfrom tungstenkit.io import BaseIO, Image\nfrom tungstenkit.model import TungstenModel, config\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@config(\ngpu=True,\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\ndescription=\"A torch model\"\n)\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\nself.model = torch.load(\"./weights.pth\")\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = preprocess(inputs)\noutput_tensor = self.model(input_tensor)\noutputs = postprocess(output_tensor)\nreturn outputs\n</code></pre> Now you can start building: <pre><code>$ tungsten build -n tungsten-example\n\n\u2705 Successfully built tungsten model: 'tungsten-basic:latest'\n</code></pre></p>"},{"location":"#run-it-as-a-restful-api-server","title":"Run it as a RESTful API server","text":"<p>The built container can be run as a standardized RESTful API server itself.</p> <p>Run the container:</p> <pre><code>$ docker run -p 3000:3000 tungsten-example:latest\n\nINFO:     Setting up the model\nINFO:     Getting inputs from the input queue\nINFO:     Starting the prediction service\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n</code></pre> <p>Now you can run predictions using the server. For example, <pre><code>$ curl -X 'POST' 'http://localhost:3000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '[{\"image\": \"https://picsum.photos/200.jpg\"}]'\n{\n    \"status\": \"success\",\n    \"outputs\": [{\"score\": 0.5, \"label\": \"dog\"}],\n    \"error_message\": null\n}\n</code></pre></p> <p>Also, a Swagger documentation for the server is automatically generated.</p> <p>Visit http://localhost:3000/docs in a browser:</p> <p></p>"},{"location":"#run-it-as-a-gui-application","title":"Run it as a GUI application","text":"<p>You can run a GUI app in a single command: <pre><code>$ tungsten demo tungsten-example:latest -p 8080\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n</code></pre> Now you can visit http://localhost:8080 in a browser and run a prediction:</p> <p></p>"},{"location":"#run-it-as-a-serverless-function","title":"Run it as a serverless function","text":"<p>Push a model to a Tungsten platform: <pre><code>$ tungsten push exampleuser/exampleproject -n tungsten-example:latest\n\n\u2705 Successfully pushed to 'https://server.tungsten-ai.com'\n</code></pre></p> <p>Now you can run predictions for the model in the Tungsten platform.</p> <p>Visit https://tungsten-ai.com in a browser:</p> <p></p>"},{"location":"#tungsten-platform","title":"Tungsten Platform","text":"<p>The Tungsten platform is where you store, run, compare, and test Tungsten models.</p>"},{"location":"#key-features_1","title":"Key Features","text":"<ul> <li>Automatic serverless deployment</li> <li>Allow your own machines to be used to run models</li> <li>Model, test data, and test spec versioning (comming soon)</li> <li>Automatically keep evaluation scores up-to-date (comming soon)</li> </ul>"},{"location":"#take-the-tour_1","title":"Take the tour","text":""},{"location":"#allow-your-own-machines-to-be-used-to-run-models","title":"Allow your own machines to be used to run models","text":"<p>You can register Tungsten runners to a Tungsten server and make the server use your own machines for running models.</p> <p>Register a runner:</p> <pre><code>$ tungsten-runner register\n\nEnter runner mode (pipeline, prediction) [prediction]: prediction\nEnter URL of the tungsten server: https://server.tungsten-ai.com\nEnter registration token: C6r5rp2PhfdXbJtFbBMhifgLDhagAc\nEnter runner name [mydesktop]: myrunner \nEnter tags (comma separated) []: myrunnergroup\nEnter GPU index to use []: 0\nRunner 'mjpyeon-desktop' is registered - id: 245\nUpdated runner config\n</code></pre> <p>Run all registered runners:</p> <pre><code>$ tungsten-runner run\n\nRunner 0   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n                      2023-04-21 16:59:49.184 | INFO     | Job 0f7c50867417456ebd1389cfb74e489f assigned\nRunner 1   | running  2023-04-21 16:59:14.490 | INFO     | Fetching a prediction job\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Apache License 2.0.</p>"},{"location":"tutorial/getting_started/","title":"Getting Started","text":""},{"location":"tutorial/getting_started/#installation","title":"Installation","text":"<p>The first step is to install Tungstenkit.</p> <p>The prerequisites for installing tungstenkit are:</p> <ul> <li>Python &gt;= 3.7</li> <li>Docker</li> </ul> <p>If they are ready, you can install Tungstenkit as follows:</p> <pre><code>pip install tungstenkit\n</code></pre>"},{"location":"tutorial/getting_started/#run-an-example-model","title":"Run an example model","text":""},{"location":"tutorial/getting_started/#create-a-directory","title":"Create a directory","text":"<p>Let's start by creating a working directory: <pre><code>mkdir tungsten-quickstart\ncd tungsten-quickstart\n</code></pre></p>"},{"location":"tutorial/getting_started/#build-a-model","title":"Build a model","text":"<p>To build a Tungsten model, you should define your input, output, setup &amp; predict functions, and dependencies in <code>tungsten_model.py</code> file.</p> <p>For example, you can define them for an image classification model like this: <pre><code>from typing import List\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights, MobileNetV2\nfrom tungstenkit.io import BaseIO, Image\nfrom tungstenkit.model import TungstenModel, config\nclass Input(BaseIO):\nimage: Image\nclass Output(BaseIO):\nscore: float\nlabel: str\n@config(\ndescription=\"Image classification model\",\npython_packages=[\"torch\", \"torchvision\"],\nbatch_size=64,\n)\nclass Model(TungstenModel[Input, Output]):\ndef setup(self):\nself.model = MobileNetV2()\nself.model.load_state_dict(torch.load(\"mobilenetv2_weights.pth\"))\nself.model.eval()\nself.labels = MobileNet_V2_Weights.IMAGENET1K_V2.meta[\"categories\"]\nself.transforms = transforms.Compose(\n[\ntransforms.Resize((224, 224)),\ntransforms.PILToTensor(),\ntransforms.ConvertImageDtype(torch.float),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n]\n)\ndef predict(self, inputs: List[Input]) -&gt; List[Output]:\ninput_tensor = self._preprocess(inputs)\nsoftmax = self.model(input_tensor).softmax(1)\nscores, class_indices = torch.max(softmax, 1)\npred_labels = [self.labels[idx.item()] for idx in class_indices]\nreturn [\nOutput(score=score.item(), label=label) for score, label in zip(scores, pred_labels)\n]\ndef _preprocess(self, inputs: List[Input]) -&gt; torch.Tensor:\npil_images = [inp.image.to_pil_image() for inp in inputs]\ntensors = [self.transforms(img) for img in pil_images]\ninput_tensor = torch.stack(tensors, dim=0)\nreturn input_tensor\n</code></pre> Copy that to a file <code>tungsten_model.py</code>.</p> <p>To setup a model, you should download model weights: <pre><code>curl -o mobilenetv2_weights.pth https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\n</code></pre></p> <p>Now, you can build a model using the definition: <pre><code>tungsten build\n</code></pre></p> <p>You can see that the model you've just created is added to the model list: <pre><code>tungsten models\n</code></pre></p>"},{"location":"tutorial/getting_started/#run-locally","title":"Run locally","text":"<p>Now, you can test the model in your local machine by running predictions.</p> <p>Tungstenkit provides multiple options for that.</p>"},{"location":"tutorial/getting_started/#option-1-an-interactive-web-demo","title":"Option 1: an interactive web demo","text":"<p><pre><code>tungsten demo -p 8080\n</code></pre> Visit http://localhost:8080 to check.</p>"},{"location":"tutorial/getting_started/#option-2-a-restful-api","title":"Option 2: a RESTful API","text":"<p><pre><code>tungsten serve -p 3000\n</code></pre> Visit http://localhost:3000/docs to check.</p>"},{"location":"tutorial/getting_started/#run-remotely","title":"Run remotely","text":"<p>To do this, you should have an account and an entered project in a Tungsten server running at https://tungsten-ai.com.  </p> <p>If you have them, let's login first. <pre><code>tungsten login\n</code></pre></p> <p>Then, you can push the built model: <pre><code>tungsten push &lt;username&gt;/&lt;project name&gt;\n</code></pre></p> <p>Now you can find a new model is added to the project.</p> <p>Visit https://tungsten-ai.com in a browser and run it.</p> <p>Also, you can pull the model as follows: <pre><code>tungsten pull &lt;username&gt;/&lt;project name&gt;:&lt;model version&gt;\n</code></pre></p>"},{"location":"tutorial/getting_started/#upgrade-the-example","title":"Upgrade the example","text":""},{"location":"tutorial/inputs_and_outputs/","title":"Inputs and Outputs","text":""},{"location":"tutorial/inputs_and_outputs/#the-baseio-class","title":"The BaseIO class","text":""},{"location":"tutorial/inputs_and_outputs/#files","title":"Files","text":""},{"location":"tutorial/inputs_and_outputs/#supported-types","title":"Supported types","text":""},{"location":"tutorial/tungsten_model/","title":"The Tungsten Model","text":""},{"location":"tutorial/tungsten_model/#declare-input-output-types","title":"Declare input &amp; output types","text":""},{"location":"tutorial/tungsten_model/#define-how-a-prediction-works","title":"Define how a prediction works","text":""},{"location":"tutorial/tungsten_model/#define-how-to-setup-a-model","title":"Define how to setup a model","text":""},{"location":"tutorial/tungsten_model/#configure-a-model","title":"Configure a model","text":""}]}